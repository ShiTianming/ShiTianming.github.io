<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[YARN架构原理详解]]></title>
    <url>%2F2018%2F07%2F30%2FYARN%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[YARN基本组件YARN的基本理念是将资源管理和任务调度/监视分成两个独立的服务：全局的资源管理器（ResoureManager）和单独任务的ApplicationMaster。 一个应用既是单独的一个任务，也是一个DAG的任务。YARN总体上仍然是master/slave结构，在整个资源管理框架中，ResourceManager是master，NodeManager是slave。ResourceManager负责对各个NodeManager上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，其负责向ResourceManager申请资源，并要求NodeManager启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此他们之间不会互相影响。 ResourceManger]]></content>
      <categories>
        <category>大数据</category>
        <category>yarn</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala之模式匹配]]></title>
    <url>%2F2018%2F07%2F25%2Fscala%E4%B9%8B%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[简介模式可以当做对某个类型，其内部数据在结构上抽象出来的表达式。scala中模式匹配使用match关键字。match可以当做是java风格的switch的广义化。但是有三个区别： scala中的match是一个表达式，可以匹配各种情况； scala的可选分支不会贯穿到下一个case； 如果一个模式都没匹配上，会抛出MatchError的异常。一般会添加什么都不做的缺省case。 模式的种类通配模式通配模式(_)可以匹配任何对象。1234567def judgeGrade(grade:String): Unit = &#123; grade match &#123; case _ =&gt; println("others") &#125;&#125;judgeGrade("hello") //others 常量模式常量模式仅匹配自己。任何字面量都可以作为常量模式使用。1234567891011def judgeGrade(grade:String): Unit = &#123; grade match &#123; case "A" =&gt; println("A") case "B" =&gt; println("B") case "C" =&gt; println("C") case _ =&gt; println("others") &#125;&#125;judgeGrade("A") //AjudgeGrade("B") //B 变量模式变量模式匹配任何对象。这一点和通配模式相同。不同之处在于变量模式会将对应的变量绑定在匹配的对象上。之后可以用这个变量对对象作进一步的处理。123456789def matchSomething(something: Int): Unit = &#123; something match &#123; case 0 =&gt; println("zero") case something =&gt; println("not zero: " + something) &#125;&#125;matchSomething(0) // zeromatchSomething(1) // not zero: 1 上例中something可以匹配任何除0外的Int值。另外例子：12345678910import math.&#123;E, Pi&#125;def matchPi(x: Double): Unit = &#123; x match &#123; case Pi =&gt; println("Pi: " + Pi) case _ =&gt; println("OK") &#125;&#125;matchPi(E) // OK 可以看出E并不匹配Pi。scala采用了一个简单的词法来区分：一个以小写字母打头的简单名称会被当做模式变量处理，所有其他引用都是常量。12345678910import math.&#123;E, Pi&#125;def matchpi(x: Double): Unit = &#123; x match &#123; case pi =&gt; println("Pi: " + pi) case _ =&gt; println("OK") &#125;&#125;matchPi(E) // Pi: 2.718281828459045 构造方法模式如例所示。假设匹配的是一个样例类（case class），这样模式将首先检查被匹配的对象是否是以这个名称命名的样例类的实例，然后再检查这个对象的改造方法参数是否匹配这些额外给出的模式。如果不是样例类，则需要定义伴生对象并实现unapply方法。12345678910111213141516case class Person(name: String, age: Int)object ConstructorPattern &#123; def main(args: Array[String]): Unit = &#123; val p = Person("stm", 25) def constructorPattern(p: Person) = &#123; p match &#123; case Person(name, age) =&gt; println("name: " + name + ", age: " + age) case _ =&gt; "other" &#125; &#125; constructorPattern(p) // name: stm, age: 25 &#125;&#125; 序列模式可以和Array、List等序列类型匹配。在模式中可以给出任意数量的元素。其原理也是通过case class。_*可以匹配剩余元素，包括0个。123456789101112131415object SequencePattern &#123; def main(args: Array[String]): Unit = &#123; val list = List("spark", "hive") val arr = Array("scala", "java", "python") def sequencePattern(p: Any) = p match &#123; case List(_, second, _*) =&gt; println(second) case Array(first, second, _*) =&gt; println(first + ", " + second) case _ =&gt; println("other") &#125; sequencePattern(list) // hive sequencePattern(arr) // scala, java &#125;&#125; 元组模式元组模式用于匹配scala中的元组内容。_*不适用于元组。1234567891011121314object TuplePattern &#123; def main(args: Array[String]): Unit = &#123; val tuple1 = ("spark", "hive", "hadoop") val tuple2 =("java", "python") def tuplePattern(t:Any) = t match &#123; case (one, _, _) =&gt; println(one) case (one, two) =&gt; println(one + ", " + two) case _ =&gt; println("other") &#125; tuplePattern(tuple1) // spark tuplePattern(tuple2) // java, python &#125;&#125; 类型模式可以用来替代类型测试和类型转换。Map[_, _]匹配任意Map。123456789101112object TypePattern &#123; def main(args: Array[String]): Unit = &#123; def typePattern(t:Any) = t match &#123; case t :String =&gt; println("t.length: " + t.length) case t :Map[_, _] =&gt; println("t.size: " + t.size) case _ =&gt; println("other") &#125; typePattern("hello") // t.length: 5 typePattern(Map(1-&gt;'a', 2-&gt;'b')) // t.size: 2 &#125;&#125; 类型擦除java和scala中都采用了擦除式的泛型。即在运行中无法判定某个给定的Map对象是用两个Int类型参数创建还是其他类型。但是数组除外。123456789101112131415161718192021object TypeErasure &#123; def main(args: Array[String]): Unit = &#123; val m1 = Map(1 -&gt; 1, 2 -&gt; 2) val m2 = Map(1 -&gt; "a", 2-&gt; "b") def isIntIntMap(x:Any) = x match &#123; case m:Map[Int, Int] =&gt; println(true) case _ =&gt; println(false) &#125; isIntIntMap(m1) //true isIntIntMap(m2) //true def isStringArray(x:Any) = x match &#123; case x : Array[String] =&gt; println(true) case _ =&gt; println(false) &#125; isStringArray(Array("hello")) //true isStringArray(Array(33)) //false &#125;&#125; 变量绑定模式可以对任何其他模式添加变量。只需要写下变量名、一个@符合模式本身，就得到一个变量绑定模式。如果匹配成功，就将匹配的对象赋值给这个变量。123456789101112object VariableBindingPattern &#123; def main(args: Array[String]): Unit = &#123; val t = List(List(1,2), List(4,5,6)) def variableBindingPattern(t:Any) = t match &#123; case List(_, e@List(_, _, _)) =&gt; println(e) case _ =&gt; println("other") &#125; variableBindingPattern(t) //List(4, 5, 6) &#125;&#125; 模式守卫使用if表达式。1234567891011121314object PatternGuards &#123; def main(args: Array[String]): Unit = &#123; val list1 = List(1, 2, 3) val list2 = List(4, 5, 6) def patternGuards(x: Any) = x match &#123; case List(first, _*) if first == 1 =&gt; println(x) case _ =&gt; println("others") &#125; patternGuards(list1) //List(1, 2, 3) patternGuards(list2) //others &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala case 之类与对象]]></title>
    <url>%2F2018%2F07%2F24%2Fscala-case-%E4%B9%8B%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍case class与普通class的一些区别。以及case class与case object的异同点。 case class与class的区别 Person.scala源码，运行后生成Person$.class和Person.class两个文件。12345678case class Person(age:Int, name:String)object Person &#123; def main(args: Array[String]): Unit = &#123; val person = Person(25,"stm") println(person.toString) &#125;&#125; 对Person.class反编译如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import scala.Function1;import scala.Option;import scala.Product;package com.stm.datastructures.caseClass;class;import scala.Serializable;import scala.Tuple2;import scala.collection.Iterator;import scala.reflect.ScalaSignature;import scala.runtime.BoxesRunTime;import scala.runtime.ScalaRunTime.;import scala.runtime.Statics;@ScalaSignaturepublic class Person implements Product, Serializable &#123; private final int age; private final String name; public Person(int age, String name) &#123; Product.class.$init$(this); &#125; public boolean equals(Object x$1) &#123; if (this != x$1) &#123; Object localObject = x$1; int i; if ((localObject instanceof Person)) &#123; i = 1; &#125; else &#123; i = 0; &#125; if (i == 0) &#123; break label96; &#125; Person localPerson = (Person) x$1; if (age() == localPerson.age()) &#123; str = localPerson.name(); String tmp54_44 = name(); if (tmp54_44 == null) &#123; tmp54_44; if (str == null) &#123; break label75; &#125; tmpTernaryOp = tmp54_44; break label88; &#125; &#125; &#125; &#125; public String toString() &#123; return ScalaRunTime..MODULE$._toString(this); &#125; public int hashCode() &#123; int i = -889275714; i = Statics.mix(i, age()); i = Statics.mix(i, Statics.anyHash(name())); return Statics.finalizeHash(i, 2); &#125; public boolean canEqual(Object x$1) &#123; return x$1 instanceof Person; &#125; public Iterator&lt;Object&gt; productIterator() &#123; return ScalaRunTime..MODULE$.typedProductIterator(this); &#125; public Object productElement(int x$1) &#123; int i = x$1; switch (i) &#123; default: throw new IndexOutOfBoundsException(BoxesRunTime.boxToInteger(x$1).toString()); case 1: break; &#125; return BoxesRunTime.boxToInteger(age()); &#125; public int productArity() &#123; return 2; &#125; public String productPrefix() &#123; return "Person"; &#125; public String copy$default$2() &#123; return name(); &#125; public int copy$default$1() &#123; return age(); &#125; public Person copy(int age, String name) &#123; return new Person(age, name); &#125; public String name() &#123; return this.name; &#125; public int age() &#123; return this.age; &#125; public static Function1&lt;Object, Function1&lt;String, Person&gt;&gt; curried() &#123; return Person..MODULE$.curried(); &#125; public static Function1&lt;Tuple2&lt;Object, String&gt;, Person&gt; tupled() &#123; return Person..MODULE$.tupled(); &#125; public static Person apply(int paramInt, String paramString) &#123; return Person..MODULE$.apply(paramInt, paramString); &#125; public static Option&lt;Tuple2&lt;Object, String&gt;&gt; unapply(Person paramPerson) &#123; return Person..MODULE$.unapply(paramPerson); &#125;&#125; 对Person$.class编译如下：12345678910111213141516171819202122232425262728293031323334353637import scala.Option;import scala.Serializable;import scala.Some;import scala.Tuple2;import scala.runtime.AbstractFunction2;import scala.runtime.BoxesRunTime;public final class Person$ extends AbstractFunction2&lt;Object, String, Person&gt; implements Serializable &#123; public static final MODULE$; private Person$() &#123; MODULE$ = this; &#125; private Object readResolve() &#123; return MODULE$; &#125; public Option&lt;Tuple2&lt;Object, String&gt;&gt; unapply(Person x$0) &#123; return x$0 == null ? None..MODULE$: new Some(new Tuple2(BoxesRunTime.boxToInteger(x$0.age()), x$0.name())); &#125; public Person apply(int age, String name) &#123; return new Person(age, name); &#125; public final String toString() &#123; return "Person"; &#125; static &#123; new (); &#125;&#125; Student.scala源码，运行后生成Student$.class和Student.class两个文件。12345678class Student(val age:Int, val name:String)&#123;&#125;object Student&#123; def main(args: Array[String]): Unit = &#123; val stu = new Student(25, "stm") println(stu) &#125;&#125; 对Student.class反编译如下:12345678910111213141516171819202122import scala.reflect.ScalaSignature;@ScalaSignaturepublic class Student &#123; private final int age; private final String name; public Student(int age, String name) &#123; &#125; public String name() &#123; return this.name; &#125; public int age() &#123; return this.age; &#125; public static void main(String[] paramArrayOfString) &#123; Student..MODULE$.main(paramArrayOfString); &#125;&#125; 对Student$.class反编译如下:123456789101112131415161718import scala.Predef.;public final class Student$ &#123; public static final MODULE$; static &#123; new (); &#125; public void main(String[] args) &#123; Student stu = new Student(25, "stm"); Predef..MODULE$.println(stu); &#125; private Student$() &#123; MODULE$ = this; &#125;&#125; 从编译结果，可以看出一下几点： 生成了apply方法，可以直接把对象当做方法使用。 1val person = Person(25,"stm") 继承了Product和Serializable。 age和name是用final修饰的。 默认实现了toString。 实现了name()和age()方法。（参数列表中的参数都隐式获得一个val作为前缀） 12println(person1.age) //25println(person1.name) //stm 重写了equals方法，比较structure而不是reference: 123val person1 = Person(25, "stm")val person2 = Person(25, "stm")println(person1 == person2) //true case object与case class区别 case object反编译后没有apply和unapply方法，因为caes object没有参数。 总结 case class和case object本质没区别，有参使用case class，无参使用case object; case class和case object增加了继承和方法。 case class和case object支持模式匹配。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark之RDD基础简介]]></title>
    <url>%2F2018%2F07%2F19%2Fspark%E4%B9%8BRDD%E5%9F%BA%E7%A1%80%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[RDD简介RDD全称为弹性分布式数据集（Resilient Distributed Dataset），是spark的编程模型，是MapReduce模型的扩展和延伸，可以在并行计算阶段高效地进行数据共享。 RDD基础RDD类型RDD主要分为以下四种类型： 创建操作：用于RDD创建工作。主要有两种方法：来自于内存集合和外部存储系统；通过转换操作生成的RDD。 转换操作：将RDD通过一定的操作变换成新的RDD。 控制操作：进行RDD持久化。让RDD根据不同的存储策略保存在内存或者磁盘中。 行动操作：能够触发spark运行的操作。spark中行动操作分为两类：一类的操作结果变成scala集合或变量，另一类的操作是将RDD保存到外部文件系统或者数据库中。 创建RDD 并行化集合创建操作 使用SparkContext的parallelize方法，在一个已经存在的scala集合上创建。集合的对象将会被复制，创建出一个可以被并行操作的分布式数据集。 外部存储创建操作 spark可以将任何Hadoop所支持的存储资源转换成RDD。 RDD依赖关系主要分为窄依赖和宽依赖： 窄依赖：每个父RDD的分区都至多被一个子RDD的分区使用。 宽依赖：多个子RDD的分区依赖一个父RDD的分区。 读取文件 wordmap的依赖关系是OneToOneDependency，属于窄依赖 使用reduceByKey操作对单词进行计数 wordreduce的依赖关系是ShuffleDependency，属于宽依赖 RDD的分区数RDD划分很多的分区（partition）分布到集群的节点中，分区的多少涉及对这个RDD进行并行计算的粒度。分区是个逻辑概念。用户可以在读取文件时指定分区数目。默认数值是改程序所分配到的CPU核数，如果从HDFS进行创建，则默认为文件的副本数。 RDD分区计算（Iterator）spark中RDD计算是以分区为单位的，而且计算函数都是对迭代器复合，不需要保存每次计算的结果。分区计算一般使用mapPartitions等操作进行。1def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] f为输入函数，处理每个分区里面的内容。 函数iterfunc把分区中一个元素和他的下个元素组成一个Turple RDD分区函数（Partitioner）spark默认提供两种划分器：哈希分区划分器（HashPartitioner）和范围分区划分器（RangePartitioner），且Partitioner只存在(K, V)类型的RDD中，对于非(K, V)类型的RDD，其Partitioner值为None。 参数4是group_rdd最终拥有的分区数 RDD基本转换操作 map： flatMap： distinct： coalesce：对RDD重新分区。第一个参数是重分区的数目，第二个参数为是否进行shuffle，默认为false。如果重分区的数目大于原分区，则需要设为True。 repartition：是coalesce第二个参数为True时的实现。 randomSplit： union：结果不去重。 mapPartitions mapPartitionsWithIndex zip：将两个同样分区RDD进行合并，键值分别对照组合。分区不同的两个RDD会报异常。 RDD键值转换操作 reduceByKey：将RDD[K, V]中每个K对应的V根据映射函数进行计算。 reduceByKeyLocally：同reduceByKey，不过是将结果映射到一个Map[K, V]中。 join、fullOuterJoin、leftOuterJoin、rightOuterJoin： join 内连接操作： leftOuterJoin 左连接操作： rightOuterJoin 右连接操作： RDD行动操作 aggregate： 进行aggregate操作：先在每个分区中迭代执行 (x:Int, y:Int) =&gt; x + y，并且zeroValue为1，即分区1中为1+5+4+3+2+1=16，分区2中为1+10+9+8+7+6=41再将两个分区进行合并，1+16+41=58 总结本文主要对RDD的四种操作进行了简单汇总，后续将对一些操作函数进行扩充完善。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala中的map和flatMap的区别]]></title>
    <url>%2F2018%2F07%2F16%2Fscala%E4%B8%AD%E7%9A%84map%E5%92%8CflatMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Titan图形数据库学习笔记（一）]]></title>
    <url>%2F2018%2F07%2F10%2FTitan%E5%9B%BE%E5%BD%A2%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Titan数据库简介 Titan 是一个可扩展的图形数据库，结合HBase、Cassandra、BerkeleyDB提供存储功能，ES、Lucene、Solar提供索引功能，可利用Hadoop计算框架对图数据进行分析、统计。经过优化，可用于存储和索引分布于多节点集群的百亿级顶点和边的图，同时，Titan又是一个事务数据库，可以支持数千个并发用户实时执行复杂图形遍历。 框架Titan是一个图形数据库引擎，其本身专注于紧凑图表序列化，丰富的图形数据库建模和高效的查询执行。其框架主要如下: Titan与底层磁盘之间有多个存储和索引适配器： 存储： Cassandra HBase BerkeleyDB 索引： Elasticsearch Solr Lucene Titan提供了三种交互式接口： TitanGraph API TinkPop API Management API Titan数据库环境配置 下载titan-1.0.0-hadoop2.zip解压。 配置文件 进入conf/es文件夹，配置elasticsearch.yml，截取如下： 12345678path.data: db/es/datapath.work: db/es/workpath.logs: logpath.plugins: bin/espluginsnetwork.host: 22.144.110.125transport.tcp.port: 9300http.port: 9200discovery.zen.ping.unicast.hosts: ["hadoop1","hadoop2","hadoop3","hadoop4","hadoop5","hadoop6"] 进入conf文件夹，配置titan-hbase-es.properties，截取如下： 123456789storage.backend=hbasestorage.hostname=hadoop1, hadoop2, hadoop3, hadoop4, hadoop5, hadoop6cache.db-cache = truecache.db-cache-clean-wait = 20cache.db-cache-time = 180000cache.db-cache-size = 0.5index.search.backend=elasticsearchindex.search.hostname=hadoop5index.search.elasticsearch.client-only=true 启动 进入bin目录，运行elasticsearch： 重开终端，运行gremlin.sh： 输入以下脚本： 123graph = TitanFactory.open('../conf/titan-hbase-es.properties')g = graph.traversal()saturn = g.V().has('name', 'saturn').next() 参考资料 http://s3.thinkaurelius.com/docs/titan/1.0.0/getting-started.html http://database.51cto.com/art/201804/570147.htm https://blog.csdn.net/samhacker/article/details/39721131]]></content>
      <categories>
        <category>图形数据库</category>
      </categories>
      <tags>
        <tag>Titan</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于华为云的CDH配置介绍]]></title>
    <url>%2F2018%2F06%2F28%2F%E5%9F%BA%E4%BA%8E%E5%8D%8E%E4%B8%BA%E4%BA%91%E7%9A%84CDH%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[基础环境配置 说明：参考文章CDH伪分布搭建教程。此处作为补充。整个操作在root用户下进行。 安装图形化界面（非必须） 1234yum groupinstall "X Window System" #1yum groupinstall "GNOME Desktop" "Graphical Administration Tools" #2ln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target #3reboot #4 安装Anaconda 说明：考虑到多用户使用存在的权限问题，将安装路径设置为/usr/local/anaconda3。 更改权限： 1chmod 777 Anaconda3-5.1.0-Linux-x86_64.sh 执行安装，安装过程中手动添加安装路径/usr/local/anaconda3，选择添加path至~/.bashrc中，不安装Mircosoft vsCode。 1sh Anaconda3-5.1.0-Linux-x86_64.sh 添加spyder链接： 方便在MobaXterm中打开图形化界面。 1ln -s /usr/local/anaconda3/bin/spyder /usr/bin/spyder 重启： 1reboot 添加hive服务 添加hive所需库： 1234567-- 在mysql中执行create user 'hive'@'%' identified by 'Password3#';grant all on *.* to 'hive'@'%' identified by 'Password3#';flush privileges;create database metastore;alter database hive character set latin1; 添加zookeeper服务 添加hive服务，数据库选择metastore 添加spark2服务 准备文件 spark2 Anaconda 安装步骤 将SPARK2_ON_YARN-2.1.0.cloudera2.jar拷贝到/opt/cloudera/csd，并且更改用户权限chown cloudera-scm:cloudera-scm 将其他文件拷贝到/opt/cloudera/parcel-repo 关闭CDH集群，重启cm server和cm agent，启动CDH集群: 12service cloudera-scm-server restartservice cloudera-scm-agent restart 进入cm页面，选择Hosts——&gt;Parcels: 按照提示进行分配安装，激活： 点击集群，添加spark2服务。 在python中正常import pyspark 在/etc/profile中添加如下配置： 过程中遇到的问题 问题1：安装成功后运行pyspark代码报错：启动spark-shell报无法获取资源： 查到的资料： https://stackoverflow.com/questions/30828879/application-report-for-application-state-accepted-never-ends-for-spark-submi/42324377 http://www.cnblogs.com/zlslch/p/6683814.html http://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/spark-shell-stuck/td-p/57603 https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/Endless-INFO-Client-Application-report-for-application-xx/m-p/31461 其他 centOS7 端口占用解决 12ss -lnp|grep 4040kill -9 pid]]></content>
      <categories>
        <category>CDH环境配置</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>CDH</tag>
        <tag>华为云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github&hexo博客搭建教程]]></title>
    <url>%2F2018%2F06%2F28%2Fgithub-hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CDH伪分布搭建教程]]></title>
    <url>%2F2018%2F06%2F27%2FCDH%E4%BC%AA%E5%88%86%E5%B8%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[基础环境配置 配置hosts 12sudo vim /etc/hosts 192.168.137.134 master 关闭防火墙 12345sudo systemctl stop firewalld.service #停止firewallsudo systemctl disable firewalld.service #禁止firewall开机启动sudo /etc/sysconfig/selinux SELINUX=disabled #修改sudo setenforce 0 配置无密码登陆 配置本地yum源 1234567891011sudo mkdir /usr/local/src #1. 将CentOS-7-x86_64-DVD-1611.iso拷贝至此sudo mkdir /usr/local/media #2.sudo mount -o loop /usr/local/src/CentOS-7-x86_64-DVD-1611.iso /usr/local/mediaCentOS7/ #3.vim /etc/yum.repos.d/CentOS7-Localsource.repo #4. 并添加一下内容 [CentOS7-Localsource] name=CentOS7 baseurl=file:///usr/local/media/CentOS7 enabled=1 gpgcheck=0sudo yum clean all #5.sudo yum makecache #6. 安装JDK 卸载openJDK 12sudo rpm -qa | grep javasudo yum remove java* 安装Oracle JDK 12345678tar xvf jdk-8u144-linux-x64.gz #路径：/usr/javasudo vim /etc/profile # 末尾添加 export JAVA_HOME=/usr/java/jdk1.8.0_144 export CLASSPATH=.:$CLASSPTAH:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/binsource /etc/profile 安装成功显示 拷贝JDBC驱动包 1cp mysql-connector-java.jar /usr/share/java #路径需要创建 时区配置 查看时区 1date -R 若不为+0800则更改时区为上海 1sudo ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 安装mysql数据库 安装mysql 1234567891011wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpmsudo yum localinstall mysql57-community-release-el7-8.noarch.rpmsudo yum install -y mysql-community-serversudo systemctl start mysqldsudo systemctl enable mysqldsudo systemctl daemon-reload 更改密码 创建cm配置过程中所需的库 配置用户权限 CM安装 在节点安装CM 拷贝CM相关文件至指定目录 安装 12sudo yum localinstall --nogpgcheck *.rpmsudo ./scm_prepare_database.sh mysql -hmaster -uamon -pPassword3# --scm-host master scm scm Password3# CDH服务安装 启动服务 12sudo service cloudera-scm-server startsudo service cloudera-scm-agent start 准备安装文件 安装cdh 登录master:7180，显示如下界面，user/password:admin/admin 选择试用版 选择节点 安装cdh 遇到的问题 问题1： 安装yarn的过程中出错： 解决方案 更改目录权限]]></content>
      <categories>
        <category>CDH环境配置</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
</search>
