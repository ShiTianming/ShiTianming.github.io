<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[orc存储格式简介]]></title>
    <url>%2F2018%2F08%2F24%2FORC%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[parquet存储格式简介]]></title>
    <url>%2F2018%2F08%2F23%2Fparquet%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[scala之传名参数和传值参数]]></title>
    <url>%2F2018%2F08%2F02%2Fscala%E4%B9%8B%E4%BC%A0%E5%90%8D%E5%8F%82%E6%95%B0%E5%92%8C%E4%BC%A0%E5%80%BC%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[简介传值参数（by-value parameter） 在函数调用之前表达式会被求值，例如Int，Long等数值参数类型；传名参数（by-name parameter） 在函数调用前表达式不会被求值，而是作为一个匿名函数传递。在介绍两者区别之前，先说一下 =&gt; 的用法。 =&gt; 用法=&gt; 用法主要有以下几点： 对于值，相当于lambda表达式： 12scala&gt; List(1, 2, 3).map&#123;(x: Int) =&gt; x * 2&#125;res0: List[Int] = List(2, 4, 6) 对于类型，=&gt; 用于分开两端的类型。 1234567891011scala&gt; val test: Function1[Int, String] = myInt =&gt; "myInt: " + myInt.toStringtest: Int =&gt; String = &lt;function1&gt;scala&gt; test(10)res0: String = myInt: 10scala&gt; val test: Int =&gt; String = myInt =&gt; "myInt: " + myInt.toStringtest: Int =&gt; String = &lt;function1&gt;scala&gt; test(10)res1: String = myInt: 10 如果参数为空，即 =&gt; 左边为空，形式为 ()=&gt;T： 1234567891011scala&gt; val test: Function0[String] = () =&gt; "Hello world"test: () =&gt; String = &lt;function0&gt;scala&gt; test()res11: String = Hello worldscala&gt; val test: () =&gt; String = () =&gt; "Hello world"test: () =&gt; String = &lt;function0&gt;scala&gt; test()res12: String = Hello world 如果无返回值，即形式为 A[,B,…]=&gt;Unit： 1234567891011scala&gt; val test: Function1[Int, Unit] =myInt =&gt; println(myInt)test: Int =&gt; Unit = &lt;function1&gt;scala&gt; test(10)10scala&gt; val test: Int =&gt; Unit =myInt =&gt; println(myInt)test: Int =&gt; Unit = &lt;function1&gt;scala&gt; test(10)10 如果无参数无返回值，即形式为 []=&gt;Unit： 1234567891011scala&gt; val test: Function0[Unit] = () =&gt; println("Hello world")test: () =&gt; Unit = &lt;function0&gt;scala&gt; test()Hello worldscala&gt; val test: () =&gt; Unit = () =&gt; println("Hello world")test: () =&gt; Unit = &lt;function0&gt;scala&gt; test()Hello world 如果作为一个函数的参数的类型声明，且坐便没有符号，如def func(param: =&gt; T)。这种形式叫做传名参数； 在case语句中，=&gt; 用于分隔模式和结果表达式。 传值参数 先对表达式进行计算，然后将结果带入函数。 简单类型 123456789101112131415object Demo01 &#123; def strToInt(s: String) = &#123; println("call strToInt") s.toInt &#125; def main(args: Array[String]): Unit = &#123; strToInt(&#123; println("call by value"); "10" &#125;) &#125;&#125;//output://call by value//call strToInt 复杂类型 1234567891011121314151617181920object Demo02 &#123; def func(f: String =&gt; Int =&gt; Long): Long = &#123; println("call func") f("1")(2) &#125; def curry(s: String)(i: Int): Long = &#123; s.toLong + i.toLong &#125; def main(args: Array[String]): Unit = &#123; func &#123; println("call by value") curry &#125; &#125;&#125;//output://call by value//call func 传名参数 参数在进入函数后，每次在函数体内调用的时候才会计算。 简单类型 1234567891011121314151617object Demo01 &#123; def strIoInt(s: =&gt; String): Int = &#123; println("call strToInt") s.toInt &#125; def main(args: Array[String]): Unit = &#123; strIoInt(&#123; println("call by name") "10" &#125; ) &#125;&#125;// output:// call strToInt// call by name 复杂类型 1234567891011121314151617181920object Demo02 &#123; def func(f: =&gt; String =&gt; Int =&gt; Long): Long = &#123; println("call func") f("1")(2) &#125; def curry(s: String)(i: Int): Long = &#123; s.toLong + i.toLong &#125; def main(args: Array[String]): Unit = &#123; func &#123; println("call by name") curry &#125; &#125;&#125;// output:// call func// call by name 总结 () =&gt; T 可以简写成 =&gt; T。 示例-1 123456789101112131415161718192021object Demo03 &#123; def strIoInt(s: () =&gt; String): Int = &#123; println("call strToInt") s().toInt &#125; def main(args: Array[String]): Unit = &#123; // 注意调用的顺序 strIoInt(&#123; println("call by value") () =&gt; &#123; println("call by name"); "10" &#125; &#125; ) &#125;&#125;// output：// call by value// call strToInt// call by name 示例-2 1234567891011121314151617181920212223242526object Demo04 &#123; def func(f: () =&gt; String =&gt; Int =&gt; Long): Long = &#123; println("call func") f()("1")(2) &#125; def curry(s: String)(i: Int): Long = &#123; s.toLong + i.toLong &#125; def main(args: Array[String]): Unit = &#123; // 注意调用的顺序 func(&#123; println("call by value") () =&gt; &#123; println("call by name") curry &#125; &#125; ) &#125;&#125;// output:// call by value// call func// call by name]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的工作机制（四）]]></title>
    <url>%2F2018%2F08%2F01%2FMapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[MapReduce之部分源码分析主要对部分重要函数源码进行分析。 OutputCommittersMapReduce使用一个提交协议确保作业和任务都完全成功或者失败。这个行为通过OutputCommitters实现。OutputCommitters的源码如下：12345678910111213141516171819202122232425262728293031323334public abstract class OutputCommitter &#123; public abstract void setupJob(JobContext jobContext) throws IOException; @Deprecated public void cleanupJob(JobContext jobContext) throws IOException &#123;&#125; public void commitJob(JobContext jobContext) throws IOException &#123; cleanupJob(jobContext); &#125; public void abortJob(JobContext jobContext, JobStatus.State state) throws IOException &#123; cleanupJob(jobContext); &#125; public abstract void setupTask(TaskAttemptContext taskContext) throws IOException; public abstract boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException; public abstract void commitTask(TaskAttemptContext taskContext) throws IOException; public abstract void abortTask(TaskAttemptContext taskContext) throws IOException; @Deprecated public boolean isRecoverySupported() &#123; return false; &#125; public boolean isRecoverySupported(JobContext jobContext) throws IOException &#123; return isRecoverySupported(); &#125; public void recoverTask(TaskAttemptContext taskContext) throws IOException &#123;&#125;&#125; setupJob()方法在作业运行前调用，通常用来执行初始化操作。 如果作业成功，就调用commitJob()方法，在默认的基于文件的实现中，它用于删除临时的工作空间并在输出目录中创建一个名为_SUCCESS的隐藏的标志文件，以此告知文件系统的客户端该作业完成。 如果作业不成功，就通过状态对象调用abortJob()，以为这该作业是否失败或终止。在默认的实现中，将删除作业的临时工作空间。 任务级别上。在任务执行之前先调用setupTask()方法，默认实现不做任何事情。因为针对任务输出命名的临时目录在写任务输出的时候被创建。 任务的提交阶段是可选的，并通过从needsTaskCommit()返回的false值关闭它。这使得执行框架不必为任务运行分布提交协议，也不需要CommitTask()或者abortTask()。 如果任务成功，就调用commitTask()，在默认的实现中它将临时的任务输出目录移动到最后的输出路径。否则，执行框架调用abortTask()，它负责删除临时的任务输出目录。 执行框架保证特定任务在有多次任务尝试的情况下，只有一个任务会被提交，其他的则被取消。]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的工作机制（三）]]></title>
    <url>%2F2018%2F08%2F01%2FMapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[MapReduce之shuffle和排序MapReduce确保每个reducer的输入都是按键排序的。系统执行排序、将map输出作为输入传给reducer的过程称为shuffle。 map端map函数开始产生输出时，利用缓冲的方式写到内存并出于效率的考虑进行预排序。 每个map任务都有一个环形内存缓冲区用于存储任务输出。默认缓冲区大小为100MB（可以通过mapreduce.task.io.sort.mb属性调整）。一旦缓冲内容达到阈值（mapreduce.map.sort.spill.percent属性，默认为0.8），一个后台线程便开始把内容溢出到磁盘。 在溢出写到磁盘过程中，map输出继续写到缓冲区。但如果在此期间缓冲区被填满，map会被阻塞直到写磁盘过程完成。溢出内容写到指定目录中（mapreduce.cluster.local.dir属性指定）。 在写磁盘之前，线程首先根据reducer把数据划分成相应的分区。在每个分区中，后台线程按键进行内存中排序，如果有一个combiner函数，它就在排序后的输出上运行。运行combiner函数使得map输出结果更紧凑，因此减少写到磁盘的数据和传递给reducer的数据。 map任务写完最后一个输出记录后，会有几个溢出文件。在任务完成之前，溢出文件被合并成一个已分区且已排序的输出文件。 如果至少存在3个溢出文件（mapreduce.map.combine.minspills属性设置）时，则combiner就会在输出文件写到磁盘之前再次运行。如果只有1或2个溢出文件，则不会为该map输出运行combiner（combiner的运行不会影响最终结果）。 将map输出写到磁盘的过程中对其压缩会加快写磁盘的速度，节约磁盘空间，并且减少传给reducer的数据量。默认不进行压缩，将mapreduce.map.output.compress设为True启用压缩，压缩格式由mapreduce.map.output.compress.codec指定。 reducer通过HTTP得到输出文件的分区。 reduce端reduce任务需要集群上若干个map任务的输出作为其分区文件。每个map任务完成时间可能不同，因此在每个任务完成时，reduce任务就开始复制其输出。这是reduce任务的复制阶段。reduce任务有少量复制线程，因此能够并行取得map输出。默认是5个线程。 reduce如何知道要从哪台机器取得map输出呢？map任务完成后，会使用心跳机制通知ApplicationMaster。因此，对于指定作业，ApplicationMaster知道map输出和主机位置之间的映射关系。reduce中的一个线程定期询问master以便获取map输出主机的位置，知道获得所有输出位置。 如果map输出相当小，会被复制到reduce的JVM中，否则被复制到磁盘中。随着磁盘上副本增多，后台线程会将他们合并为更大的、排好序的文件。（压缩的map输出都必须在内存中被解压缩） 复制完所有map输出后，reduce任务进入排序阶段（实际上是合并，排序是在map端进行的），这个阶段将合并map输出，维持其顺序排序。假如有50个map输出，合并因子是10，则会5次合并，最后生成5个中间文件。 最后，reduce阶段直接把数据输入给reduce函数，并没有将这5个文件合并成一个已排序的文件作为一趟。 在reduce阶段，对已排序输出的每个键调用reduce函数。此阶段的输出直接写到输出文件系统。]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的工作机制（二）]]></title>
    <url>%2F2018%2F08%2F01%2FMapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[MapReduce之数据流MapReduce作业是客户端需要执行的衣蛾工作单元，它包括输入数据、MapReduce程序和配置信息。Hadoop将作业分成若干个任务来执行，包括map任务和reduce任务。这些任务运行在集群的节点上，并通过YARN进行调度。 分片Hadoop将MapReduce的输入数据划分成等长的小数据块，称为输入分片（input split）。Hadoop为每个分片构建一个map任务，并由该任务来运行用户定义的map函数从而处理分片中的每条记录。对于大多数作业而言，一个合理的分片大小趋向于HDFS一个块的大小，默认是128MB。 mapHadoop在存储有输入数据的节点上运行map任务，可以避免使用集群带宽资源，提高性能（数据本地化优化data locality optimization）。map任务将其输出写入本地磁盘而非HDFS。因为map的输出是中间结果，一旦作业完成，map的结果就可以删除。如果运行map任务的节点在将map中间结果传送给reduce任务之前失败，Hadoop将在另一个节点上重新运行这个map任务以再次构建map中间结果。 reducereduce任务并不具备数据本地化的优势，单个reduce任务的输入通常来自于所有的mapper的输出。数据在reduce端合并，然后由用户定义的reduce函数处理。reduce输出的结果通常存储在HDFS上。reduce任务的数量并非由输入数据的大小决定，而是独立指定的。如果有多个reduce任务，每个map任务就会针对输出进行分区，即为每个reduce任务建一个分区。每个分区有许多键（及其对应的值），但每个键对应的键-值对记录都在同一分区中。默认通过哈希函数分区。 一个reduce任务的MapReduce数据流 多个reduce任务的MapReduce数据流 当数据处理完全并行，无需shuffle combiner函数集群上的可用带宽限制了MapReduce作业的数量，因此避免map和reduce任务之间的数据传输是有利的。Hadoop允许用户针对map任务的输出指定一个combiner，其输出作为reduce函数的输入。combiner属于优化方案，所以Hadoop无法确定要对一个指定的map任务输出记录调用多少次combiner。不管调用多少次，reduce的结果都是一样的。]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的工作机制（一）]]></title>
    <url>%2F2018%2F08%2F01%2FMapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[MapReduce作业简介MapReduce作业通过Job对象的submit()方法来调用。整个过程大概可以描述为： 客户端，提交MapReduce作业； YARN资源管理器，负责协调集群上计算机资源的分配； YARN节点管理器，负责启动和监视集群中机器的计算容器（container）； MapReduce的ApplicationMaster，负责协调运行MapReduce作业的任务。它和MapReduce任务在容器中运行，这些容器由资源管理分配并由节点管理器进行管理； 分布式文件系统，用来与其他实体间共享作业文件。 MapReduce运行流程作业的提交Job的submit()方法创建了一个JobSubmitter实例，该实例调用submitJobInternal()方法，该方法用于将任务提交到集群。查看源码注释得知作业提交过程如下： 向资源管理器请求一个应用ID，用于MapReduce作业的ID（步骤2）； 检查作业的输入输出路径； 计算作业的输入分片； 设置运行作业所需要的信息； 复制作业的jar包、配置文件和j算所得的输入分片复制到以ID命名的分布式文件系统中（步骤3）; 通过调用YARN中的submitApplication()方法提交作业。并监视器状态（步骤4）。 作业的初始化YARN收到submitApplication()调用消息后，便将请求传递给调度器（scheduler）。调度器分配一个容器，然后资源管理器在节点管理器的管理下载容器中启动ApplicationMaster进程（步骤5a和5b）。ApplicationMaster是一个java应用程序，其主类是MRAppMaster。由于将接受任务的进度和完成报告（步骤6），因此ApplicationMaster对作业的初始化时通过创建多个簿记对象以保持对做作业进度的跟踪来完成的。接下来，它接受来自共享文件系统的、在客户端计算的输入分片（步骤7）。然后对每个分片创建一个map任务对象以及由mapreduce.job.reduces属性（通过setNumReduceTasks()方法设置）确定多个renduce任务对象。任务ID在此时分配。ApplicationMaster必须决定如何运行构成MapReduce作业的各个任务。如果作业很小，就选择和自己在同一个JVM上运行任务。当ApplicationMaster判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时就会发生这一情况。这称之为uber任务运行。最后，ApplicationMaster设置OutputCommitter。 任务的分配如果作业不适合作为uber任务运行，那么ApplicationMaster就会未改作业中的所有map任务和reduce任务向资源管理器请求容器（步骤8）。首先为Map任务发出请求，知道有5%的map任务已经完成时，为reduce任务的请求才会发出（map的优先级高于reduce）。reduce任务能够在集群中任意位置运行，但是map任务的请求有着数据本地化的局限。请求也为任务指定了内存需求和CPU数。 任务的执行一旦scheduler为任务分配了一个特定节点上的container，ApplicationMaster就通过与NodeManager通信来启动container（步骤9a和9b）。该任务由类YARNChild的一个java应用程序执行。在任务运行之前，首先将任务需要的资源本地化，包括作业的配置、jar文件和所有来自分布式缓存的文件（步骤10）。最后运行map任务和reduce任务（步骤11）。每个任务动能够执行setup和commit动作，他们和任务本身在同一个JVM中运行，并由作业的OutputCommitter确定。对于基于文件的作业，提交动作将任务输出由临时位置搬移到最终位置。提交协议确保当推测执行被启用时，只有一个任务副本被提交，其他的都被取消。 任务的状态一个作业和它的每个任务都有一个状态，包括： 作业或任务的状态（运行中、成功、失败）； map和reduce的进度； 作业计数器的值； 状态消息或描述。 任务在运行时，对其进度保持追踪。对map任务，任务进度是已处理输入所占的比例；对reduce任务，系统会估计已处理reduce输入的比例。 作业的完成当ApplicationMaster收到作业最后一个任务完成的通知后，便把作业的状态设置为“成功”。于是job打印一条消息告知用户，然后从waitForCompletion()方法返回。job的统计信息和计数值也输出到控制台。]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN中的资源调度]]></title>
    <url>%2F2018%2F07%2F30%2FYARN%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[YARN中有三种调度器可用：FIFO调度器（FIFO Scheduler），容量调度器（Capacity Scheduler）和公平调度器（Fair Scheduler）。 FIFO调度器（FIFO Scheduler）FIFO调度器将应用放置在一个队列中，然后按照提交的顺序（先进先出）运行应用。首先为队列中第一个应用的请求分配资源，第一个应用的请求满足后再依次为队列中下一个应用服务。FIFO调度器优点在于简单易懂，不需要任何配置，但是不适合大集群。大的应用会占用集群中的所有资源，所以每个应用必须等待直到轮到自己运行。 容量调度器（Capacity Scheduler）使用容量调度器时，一个独立的专门队列保证小作业一提交就可以启动，由于队列容量是为那个队列中的作业所保留的，因此这种策略是以整个集群的利用率为代价的。这意味着相比于FIFO调度器，大作业执行的时间要长。容量调度器允许多个组织共享一个集群，每个组织可以分配到全部集群资源的一部分。每个组织被配置一个专门的队列，每个队列被配置为可以使用一定的集群资源。对了可以进一步按层次划分，这样每个组织内的不同用户能够共享该组织队列所分配的资源。在一个队列内，使用FIFO调度策略对应用进行调度。单个作业使用的资源不会超过其队列容量。然而，如果队列中有多个作业，导致队列资源不够用，这时如果仍有可用的空闲资源，那么容量调度器可能会将空余的资源分配给队列中的作业，哪怕这会超过队列容量。这称为“弹性队列”（queue elasticity）。正常操作时，容量调度器不会通过强行终止来抢占容器。因此，如果一个队列一开始资源够用，然后随着需求增长，资源开始不够用时，那么这个队列就只能等着其他队列释放容器资源。一般情况下，为队列设置一个最大容量限制，这样这个队列就不会过多侵占其他队列的容量了。 公平调度器（Fair SCheduler）公平调度器旨在为所有运行的应用公平分配资源。假设有两个用户A和B，分别拥有自己的队列。A启动一个作业，在B没有需求时，A会分配到全部可用资源；当A的作业仍在运行时，B启动一个作业，一段 时间后，每个作业都用到了一半的集群资源。这时，如果B启动了第二个走也且其他作业仍在运行，那么第二个作业将和B的其他作业（这里是第一个）共享资源，因此B的每个作业将占用四分之一的集群资源，而A仍继续占用一半的集群资源。最终的结果是资源在用户之间实现了公平共享。 抢占公平调度器支持抢占功能。所谓抢占，就是允许调度器终止那些占用资源超过了其公平共享份额的队列的容器，这些融资资源释放后可以分配给资源数量低于应得份额的队列。抢占会降低这个集群的效率，因为被终止的Containers需要重新执行。 延迟调度如果一个应用请求某个节点，那么极有可能此时有其他容器正在该节点上运行。实践发现，此时如果等待一小段时间（不超过几秒），能够增鸡杂在所请求的节点上分配到一个容器的机会，从而可以提高集群的效率。这个特性称为延迟调度（delay scheduling）。容量调度器和公平调度器都支持延迟调度。]]></content>
      <categories>
        <category>大数据</category>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN架构原理详解]]></title>
    <url>%2F2018%2F07%2F30%2FYARN%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[YARN基本组件YARN的基本理念是将资源管理和任务调度/监控分成两个独立的服务：全局的资源管理器（ResoureManager）和单独任务的ApplicationMaster。 一个应用既是单独的一个任务，也是一个DAG的任务。YARN总体上仍然是master/slave结构，在整个资源管理框架中，ResourceManager是master，NodeManager是slave。ResourceManager负责对各个NodeManager上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，其负责向ResourceManager申请资源，并要求NodeManager启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此他们之间不会互相影响。 ResourceMangerResourceManager是一个全局资源管理器，负责整个系统的资源管理和分配，包括处理客户端的请求、启动\监控Applicationmaster、监控NodeManager、资源的分配和调度。其主要有两个组件构成：调度器（Scheduler）和应用程序管理器(ApplicationsMaster)。 调度器调度器根据容量、队列等限制条件（如每个队列分配一定的资源、最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。该调度器是一个“纯调度器”，其不再从事任何具体应用程序相关的工作，比如不负责监控或跟踪应用的执行状态等，也不负责重新启动因应用执行失败或硬件故障而产生的失败任务。调度器仅根据各个应用的资源需求进行资源分配，资源分配单位用“资源容器”（Resource Container，简称Container）表示。Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。 应用程序管理器应用程序管理器负责整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。 ApplicationMaster管理YARN内运行的应用程序的每个实例。其主要是为应用程序申请资源并进一步分配给内部任务，负责协调来自ResoureManager的资源，并通过NodeManger监控Container的执行和资源的使用情况。 NodeManager整个集群中有多个NodeManager，负责每个节点上的资源和使用。功能： 单个节点上的资源管理和任务； 处理来自ResourceManager的命令； 处理来自ApplicationMaster的命令。 NodeManager管理着抽象容器，这些抽象容器代表着一些特定程序使用针对每个节点的资源。NodeManager定时向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态。 ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维度资源。当ApplicationManager向ResourceManager申请资源时，ResourceManager为ApplicationManager返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用Container中的资源。其是一个动态资源划分单位，是根据应用程序的需求动态生成的。 YARN的资源管理 资源调度和隔离是YARN作为一个资源管理系统，最重要最基础的两个功能。资源调度由ResourceManager完成，资源隔离由NodeManager完成。 ResourceManager将某个NodeManager上资源分配给任务后，NodeManager需按照要求为任务提供相应的资源，甚至保证这些资源具有独占性，为任务提供基础和保证。 谈及资源时，主要是内存、CPU、io三种资源。YARN目前仅支持内存、CPU两种资源管理调度。 内存资源多少决定任务的成败；CPU多少决定任务的快慢。 YARN的内存管理YARN允许用户配置每个节点上可用的物理内存资源。因为一个节点上内存会被若干个服务共享，比如一部分给YARN，一部分给hdfs等。YARN配置的只是自己可用的，配置参数主要有： yarn.nodemanager.resource.memory-mb：表示节点上YARN可以使用的物理内存总量； yarn.scheduler.minimum-allocation-mb：单个任务可以使用最小物理内存量； yarn.scheduler.maximum-allocation-mb：单个任务可以申请的最多的内存量。 YARN的CPU管理用户提交应用时，可以指定每个任务需要的虚拟CPU数。相关配置参数如下： yarn.nodemanager.resource.cpu-vcores：表示该节点上可以使用的虚拟CPU个数； yarn.scheduler.minimum-allocation-vcores：表示单个任务可以申请的最小CPU个数； yarn.scheduler.maximum-allocation-vcores：表示单个任务可以申请最多虚拟CPU个数。 YARN程序运行机制首先，客户端联系ResoureManager，要求它运行一个ApplicationManager进程；然后，ResoureManager找到一个能够在Container中启动ApplicationMaster的NodeManager（步骤2a和2b）。ApplicationMaster一旦运行起来后能做些什么依赖于应用本身。有可能是在所处的容器中简单运行一个计算，并将结果返回给客户端；或者向ResoureManager请求更多的Container（步骤3），以用于运行一个分布式计算（步骤4a和4b）。YARN本身不会为应用的各个部分彼此间通信提供任何手段。大多数重要的YARN应用使用某种形式的远程通信机制（例如Hadoop的RPC层）来向客户端传递状态更新和返回结果。]]></content>
      <categories>
        <category>大数据</category>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala之模式匹配]]></title>
    <url>%2F2018%2F07%2F25%2Fscala%E4%B9%8B%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[简介模式可以当做对某个类型，其内部数据在结构上抽象出来的表达式。scala中模式匹配使用match关键字。match可以当做是java风格的switch的广义化。但是有三个区别： scala中的match是一个表达式，可以匹配各种情况； scala的可选分支不会贯穿到下一个case； 如果一个模式都没匹配上，会抛出MatchError的异常。一般会添加什么都不做的缺省case。 模式的种类通配模式通配模式(_)可以匹配任何对象。1234567def judgeGrade(grade:String): Unit = &#123; grade match &#123; case _ =&gt; println("others") &#125;&#125;judgeGrade("hello") //others 常量模式常量模式仅匹配自己。任何字面量都可以作为常量模式使用。1234567891011def judgeGrade(grade:String): Unit = &#123; grade match &#123; case "A" =&gt; println("A") case "B" =&gt; println("B") case "C" =&gt; println("C") case _ =&gt; println("others") &#125;&#125;judgeGrade("A") //AjudgeGrade("B") //B 变量模式变量模式匹配任何对象。这一点和通配模式相同。不同之处在于变量模式会将对应的变量绑定在匹配的对象上。之后可以用这个变量对对象作进一步的处理。123456789def matchSomething(something: Int): Unit = &#123; something match &#123; case 0 =&gt; println("zero") case something =&gt; println("not zero: " + something) &#125;&#125;matchSomething(0) // zeromatchSomething(1) // not zero: 1 上例中something可以匹配任何除0外的Int值。另外例子：12345678910import math.&#123;E, Pi&#125;def matchPi(x: Double): Unit = &#123; x match &#123; case Pi =&gt; println("Pi: " + Pi) case _ =&gt; println("OK") &#125;&#125;matchPi(E) // OK 可以看出E并不匹配Pi。scala采用了一个简单的词法来区分：一个以小写字母打头的简单名称会被当做模式变量处理，所有其他引用都是常量。12345678910import math.&#123;E, Pi&#125;def matchpi(x: Double): Unit = &#123; x match &#123; case pi =&gt; println("Pi: " + pi) case _ =&gt; println("OK") &#125;&#125;matchPi(E) // Pi: 2.718281828459045 构造方法模式如例所示。假设匹配的是一个样例类（case class），这样模式将首先检查被匹配的对象是否是以这个名称命名的样例类的实例，然后再检查这个对象的改造方法参数是否匹配这些额外给出的模式。如果不是样例类，则需要定义伴生对象并实现unapply方法。12345678910111213141516case class Person(name: String, age: Int)object ConstructorPattern &#123; def main(args: Array[String]): Unit = &#123; val p = Person("stm", 25) def constructorPattern(p: Person) = &#123; p match &#123; case Person(name, age) =&gt; println("name: " + name + ", age: " + age) case _ =&gt; "other" &#125; &#125; constructorPattern(p) // name: stm, age: 25 &#125;&#125; 序列模式可以和Array、List等序列类型匹配。在模式中可以给出任意数量的元素。其原理也是通过case class。_*可以匹配剩余元素，包括0个。123456789101112131415object SequencePattern &#123; def main(args: Array[String]): Unit = &#123; val list = List("spark", "hive") val arr = Array("scala", "java", "python") def sequencePattern(p: Any) = p match &#123; case List(_, second, _*) =&gt; println(second) case Array(first, second, _*) =&gt; println(first + ", " + second) case _ =&gt; println("other") &#125; sequencePattern(list) // hive sequencePattern(arr) // scala, java &#125;&#125; 元组模式元组模式用于匹配scala中的元组内容。_*不适用于元组。1234567891011121314object TuplePattern &#123; def main(args: Array[String]): Unit = &#123; val tuple1 = ("spark", "hive", "hadoop") val tuple2 =("java", "python") def tuplePattern(t:Any) = t match &#123; case (one, _, _) =&gt; println(one) case (one, two) =&gt; println(one + ", " + two) case _ =&gt; println("other") &#125; tuplePattern(tuple1) // spark tuplePattern(tuple2) // java, python &#125;&#125; 类型模式可以用来替代类型测试和类型转换。Map[_, _]匹配任意Map。123456789101112object TypePattern &#123; def main(args: Array[String]): Unit = &#123; def typePattern(t:Any) = t match &#123; case t :String =&gt; println("t.length: " + t.length) case t :Map[_, _] =&gt; println("t.size: " + t.size) case _ =&gt; println("other") &#125; typePattern("hello") // t.length: 5 typePattern(Map(1-&gt;'a', 2-&gt;'b')) // t.size: 2 &#125;&#125; 类型擦除java和scala中都采用了擦除式的泛型。即在运行中无法判定某个给定的Map对象是用两个Int类型参数创建还是其他类型。但是数组除外。123456789101112131415161718192021object TypeErasure &#123; def main(args: Array[String]): Unit = &#123; val m1 = Map(1 -&gt; 1, 2 -&gt; 2) val m2 = Map(1 -&gt; "a", 2-&gt; "b") def isIntIntMap(x:Any) = x match &#123; case m:Map[Int, Int] =&gt; println(true) case _ =&gt; println(false) &#125; isIntIntMap(m1) //true isIntIntMap(m2) //true def isStringArray(x:Any) = x match &#123; case x : Array[String] =&gt; println(true) case _ =&gt; println(false) &#125; isStringArray(Array("hello")) //true isStringArray(Array(33)) //false &#125;&#125; 变量绑定模式可以对任何其他模式添加变量。只需要写下变量名、一个@符合模式本身，就得到一个变量绑定模式。如果匹配成功，就将匹配的对象赋值给这个变量。123456789101112object VariableBindingPattern &#123; def main(args: Array[String]): Unit = &#123; val t = List(List(1,2), List(4,5,6)) def variableBindingPattern(t:Any) = t match &#123; case List(_, e@List(_, _, _)) =&gt; println(e) case _ =&gt; println("other") &#125; variableBindingPattern(t) //List(4, 5, 6) &#125;&#125; 模式守卫使用if表达式。1234567891011121314object PatternGuards &#123; def main(args: Array[String]): Unit = &#123; val list1 = List(1, 2, 3) val list2 = List(4, 5, 6) def patternGuards(x: Any) = x match &#123; case List(first, _*) if first == 1 =&gt; println(x) case _ =&gt; println("others") &#125; patternGuards(list1) //List(1, 2, 3) patternGuards(list2) //others &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala case 之类与对象]]></title>
    <url>%2F2018%2F07%2F24%2Fscala-case-%E4%B9%8B%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍case class与普通class的一些区别。以及case class与case object的异同点。 case class与class的区别 Person.scala源码，运行后生成Person$.class和Person.class两个文件。12345678case class Person(age:Int, name:String)object Person &#123; def main(args: Array[String]): Unit = &#123; val person = Person(25,"stm") println(person.toString) &#125;&#125; 对Person.class反编译如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import scala.Function1;import scala.Option;import scala.Product;package com.stm.datastructures.caseClass;class;import scala.Serializable;import scala.Tuple2;import scala.collection.Iterator;import scala.reflect.ScalaSignature;import scala.runtime.BoxesRunTime;import scala.runtime.ScalaRunTime.;import scala.runtime.Statics;@ScalaSignaturepublic class Person implements Product, Serializable &#123; private final int age; private final String name; public Person(int age, String name) &#123; Product.class.$init$(this); &#125; public boolean equals(Object x$1) &#123; if (this != x$1) &#123; Object localObject = x$1; int i; if ((localObject instanceof Person)) &#123; i = 1; &#125; else &#123; i = 0; &#125; if (i == 0) &#123; break label96; &#125; Person localPerson = (Person) x$1; if (age() == localPerson.age()) &#123; str = localPerson.name(); String tmp54_44 = name(); if (tmp54_44 == null) &#123; tmp54_44; if (str == null) &#123; break label75; &#125; tmpTernaryOp = tmp54_44; break label88; &#125; &#125; &#125; &#125; public String toString() &#123; return ScalaRunTime..MODULE$._toString(this); &#125; public int hashCode() &#123; int i = -889275714; i = Statics.mix(i, age()); i = Statics.mix(i, Statics.anyHash(name())); return Statics.finalizeHash(i, 2); &#125; public boolean canEqual(Object x$1) &#123; return x$1 instanceof Person; &#125; public Iterator&lt;Object&gt; productIterator() &#123; return ScalaRunTime..MODULE$.typedProductIterator(this); &#125; public Object productElement(int x$1) &#123; int i = x$1; switch (i) &#123; default: throw new IndexOutOfBoundsException(BoxesRunTime.boxToInteger(x$1).toString()); case 1: break; &#125; return BoxesRunTime.boxToInteger(age()); &#125; public int productArity() &#123; return 2; &#125; public String productPrefix() &#123; return "Person"; &#125; public String copy$default$2() &#123; return name(); &#125; public int copy$default$1() &#123; return age(); &#125; public Person copy(int age, String name) &#123; return new Person(age, name); &#125; public String name() &#123; return this.name; &#125; public int age() &#123; return this.age; &#125; public static Function1&lt;Object, Function1&lt;String, Person&gt;&gt; curried() &#123; return Person..MODULE$.curried(); &#125; public static Function1&lt;Tuple2&lt;Object, String&gt;, Person&gt; tupled() &#123; return Person..MODULE$.tupled(); &#125; public static Person apply(int paramInt, String paramString) &#123; return Person..MODULE$.apply(paramInt, paramString); &#125; public static Option&lt;Tuple2&lt;Object, String&gt;&gt; unapply(Person paramPerson) &#123; return Person..MODULE$.unapply(paramPerson); &#125;&#125; 对Person$.class编译如下：12345678910111213141516171819202122232425262728293031323334353637import scala.Option;import scala.Serializable;import scala.Some;import scala.Tuple2;import scala.runtime.AbstractFunction2;import scala.runtime.BoxesRunTime;public final class Person$ extends AbstractFunction2&lt;Object, String, Person&gt; implements Serializable &#123; public static final MODULE$; private Person$() &#123; MODULE$ = this; &#125; private Object readResolve() &#123; return MODULE$; &#125; public Option&lt;Tuple2&lt;Object, String&gt;&gt; unapply(Person x$0) &#123; return x$0 == null ? None..MODULE$: new Some(new Tuple2(BoxesRunTime.boxToInteger(x$0.age()), x$0.name())); &#125; public Person apply(int age, String name) &#123; return new Person(age, name); &#125; public final String toString() &#123; return "Person"; &#125; static &#123; new (); &#125;&#125; Student.scala源码，运行后生成Student$.class和Student.class两个文件。12345678class Student(val age:Int, val name:String)&#123;&#125;object Student&#123; def main(args: Array[String]): Unit = &#123; val stu = new Student(25, "stm") println(stu) &#125;&#125; 对Student.class反编译如下:12345678910111213141516171819202122import scala.reflect.ScalaSignature;@ScalaSignaturepublic class Student &#123; private final int age; private final String name; public Student(int age, String name) &#123; &#125; public String name() &#123; return this.name; &#125; public int age() &#123; return this.age; &#125; public static void main(String[] paramArrayOfString) &#123; Student..MODULE$.main(paramArrayOfString); &#125;&#125; 对Student$.class反编译如下:123456789101112131415161718import scala.Predef.;public final class Student$ &#123; public static final MODULE$; static &#123; new (); &#125; public void main(String[] args) &#123; Student stu = new Student(25, "stm"); Predef..MODULE$.println(stu); &#125; private Student$() &#123; MODULE$ = this; &#125;&#125; 从编译结果，可以看出一下几点： 生成了apply方法，可以直接把对象当做方法使用。 1val person = Person(25,"stm") 继承了Product和Serializable。 age和name是用final修饰的。 默认实现了toString。 实现了name()和age()方法。（参数列表中的参数都隐式获得一个val作为前缀） 12println(person1.age) //25println(person1.name) //stm 重写了equals方法，比较structure而不是reference: 123val person1 = Person(25, "stm")val person2 = Person(25, "stm")println(person1 == person2) //true case object与case class区别 case object反编译后没有apply和unapply方法，因为caes object没有参数。 总结 case class和case object本质没区别，有参使用case class，无参使用case object; case class和case object增加了继承和方法。 case class和case object支持模式匹配。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark之RDD基础简介]]></title>
    <url>%2F2018%2F07%2F19%2Fspark%E4%B9%8BRDD%E5%9F%BA%E7%A1%80%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[RDD简介RDD全称为弹性分布式数据集（Resilient Distributed Dataset），是spark的编程模型，是MapReduce模型的扩展和延伸，可以在并行计算阶段高效地进行数据共享。 RDD基础RDD类型RDD主要分为以下四种类型： 创建操作：用于RDD创建工作。主要有两种方法：来自于内存集合和外部存储系统；通过转换操作生成的RDD。 转换操作：将RDD通过一定的操作变换成新的RDD。 控制操作：进行RDD持久化。让RDD根据不同的存储策略保存在内存或者磁盘中。 行动操作：能够触发spark运行的操作。spark中行动操作分为两类：一类的操作结果变成scala集合或变量，另一类的操作是将RDD保存到外部文件系统或者数据库中。 创建RDD 并行化集合创建操作 使用SparkContext的parallelize方法，在一个已经存在的scala集合上创建。集合的对象将会被复制，创建出一个可以被并行操作的分布式数据集。 外部存储创建操作 spark可以将任何Hadoop所支持的存储资源转换成RDD。 RDD依赖关系主要分为窄依赖和宽依赖： 窄依赖：每个父RDD的分区都至多被一个子RDD的分区使用。 宽依赖：多个子RDD的分区依赖一个父RDD的分区。 读取文件 wordmap的依赖关系是OneToOneDependency，属于窄依赖 使用reduceByKey操作对单词进行计数 wordreduce的依赖关系是ShuffleDependency，属于宽依赖 RDD的分区数RDD划分很多的分区（partition）分布到集群的节点中，分区的多少涉及对这个RDD进行并行计算的粒度。分区是个逻辑概念。用户可以在读取文件时指定分区数目。默认数值是改程序所分配到的CPU核数，如果从HDFS进行创建，则默认为文件的副本数。 RDD分区计算（Iterator）spark中RDD计算是以分区为单位的，而且计算函数都是对迭代器复合，不需要保存每次计算的结果。分区计算一般使用mapPartitions等操作进行。1def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] f为输入函数，处理每个分区里面的内容。 函数iterfunc把分区中一个元素和他的下个元素组成一个Turple RDD分区函数（Partitioner）spark默认提供两种划分器：哈希分区划分器（HashPartitioner）和范围分区划分器（RangePartitioner），且Partitioner只存在(K, V)类型的RDD中，对于非(K, V)类型的RDD，其Partitioner值为None。 参数4是group_rdd最终拥有的分区数 RDD基本转换操作 map： flatMap： distinct： coalesce：对RDD重新分区。第一个参数是重分区的数目，第二个参数为是否进行shuffle，默认为false。如果重分区的数目大于原分区，则需要设为True。 repartition：是coalesce第二个参数为True时的实现。 randomSplit： union：结果不去重。 mapPartitions mapPartitionsWithIndex zip：将两个同样分区RDD进行合并，键值分别对照组合。分区不同的两个RDD会报异常。 RDD键值转换操作 reduceByKey：将RDD[K, V]中每个K对应的V根据映射函数进行计算。 reduceByKeyLocally：同reduceByKey，不过是将结果映射到一个Map[K, V]中。 join、fullOuterJoin、leftOuterJoin、rightOuterJoin： join 内连接操作： leftOuterJoin 左连接操作： rightOuterJoin 右连接操作： RDD行动操作 aggregate： 进行aggregate操作：先在每个分区中迭代执行 (x:Int, y:Int) =&gt; x + y，并且zeroValue为1，即分区1中为1+5+4+3+2+1=16，分区2中为1+10+9+8+7+6=41再将两个分区进行合并，1+16+41=58 总结本文主要对RDD的四种操作进行了简单汇总，后续将对一些操作函数进行扩充完善。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala中的map和flatMap的区别]]></title>
    <url>%2F2018%2F07%2F16%2Fscala%E4%B8%AD%E7%9A%84map%E5%92%8CflatMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Titan图形数据库学习笔记（一）]]></title>
    <url>%2F2018%2F07%2F10%2FTitan%E5%9B%BE%E5%BD%A2%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Titan数据库简介 Titan 是一个可扩展的图形数据库，结合HBase、Cassandra、BerkeleyDB提供存储功能，ES、Lucene、Solar提供索引功能，可利用Hadoop计算框架对图数据进行分析、统计。经过优化，可用于存储和索引分布于多节点集群的百亿级顶点和边的图，同时，Titan又是一个事务数据库，可以支持数千个并发用户实时执行复杂图形遍历。 框架Titan是一个图形数据库引擎，其本身专注于紧凑图表序列化，丰富的图形数据库建模和高效的查询执行。其框架主要如下: Titan与底层磁盘之间有多个存储和索引适配器： 存储： Cassandra HBase BerkeleyDB 索引： Elasticsearch Solr Lucene Titan提供了三种交互式接口： TitanGraph API TinkPop API Management API Titan数据库环境配置 下载titan-1.0.0-hadoop2.zip解压。 配置文件 进入conf/es文件夹，配置elasticsearch.yml，截取如下： 12345678path.data: db/es/datapath.work: db/es/workpath.logs: logpath.plugins: bin/espluginsnetwork.host: 22.144.110.125transport.tcp.port: 9300http.port: 9200discovery.zen.ping.unicast.hosts: ["hadoop1","hadoop2","hadoop3","hadoop4","hadoop5","hadoop6"] 进入conf文件夹，配置titan-hbase-es.properties，截取如下： 123456789storage.backend=hbasestorage.hostname=hadoop1, hadoop2, hadoop3, hadoop4, hadoop5, hadoop6cache.db-cache = truecache.db-cache-clean-wait = 20cache.db-cache-time = 180000cache.db-cache-size = 0.5index.search.backend=elasticsearchindex.search.hostname=hadoop5index.search.elasticsearch.client-only=true 启动 进入bin目录，运行elasticsearch： 重开终端，运行gremlin.sh： 输入以下脚本： 123graph = TitanFactory.open('../conf/titan-hbase-es.properties')g = graph.traversal()saturn = g.V().has('name', 'saturn').next() 参考资料 http://s3.thinkaurelius.com/docs/titan/1.0.0/getting-started.html http://database.51cto.com/art/201804/570147.htm https://blog.csdn.net/samhacker/article/details/39721131]]></content>
      <categories>
        <category>图形数据库</category>
      </categories>
      <tags>
        <tag>Titan</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于华为云的CDH配置介绍]]></title>
    <url>%2F2018%2F06%2F28%2F%E5%9F%BA%E4%BA%8E%E5%8D%8E%E4%B8%BA%E4%BA%91%E7%9A%84CDH%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[基础环境配置 说明：参考文章CDH伪分布搭建教程。此处作为补充。整个操作在root用户下进行。 安装图形化界面（非必须） 1234yum groupinstall "X Window System" #1yum groupinstall "GNOME Desktop" "Graphical Administration Tools" #2ln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target #3reboot #4 安装Anaconda 说明：考虑到多用户使用存在的权限问题，将安装路径设置为/usr/local/anaconda3。 更改权限： 1chmod 777 Anaconda3-5.1.0-Linux-x86_64.sh 执行安装，安装过程中手动添加安装路径/usr/local/anaconda3，选择添加path至~/.bashrc中，不安装Mircosoft vsCode。 1sh Anaconda3-5.1.0-Linux-x86_64.sh 添加spyder链接： 方便在MobaXterm中打开图形化界面。 1ln -s /usr/local/anaconda3/bin/spyder /usr/bin/spyder 重启： 1reboot 添加hive服务 添加hive所需库： 1234567-- 在mysql中执行create user 'hive'@'%' identified by 'Password3#';grant all on *.* to 'hive'@'%' identified by 'Password3#';flush privileges;create database metastore;alter database hive character set latin1; 添加zookeeper服务 添加hive服务，数据库选择metastore 添加spark2服务 准备文件 spark2 Anaconda 安装步骤 将SPARK2_ON_YARN-2.1.0.cloudera2.jar拷贝到/opt/cloudera/csd，并且更改用户权限chown cloudera-scm:cloudera-scm 将其他文件拷贝到/opt/cloudera/parcel-repo 关闭CDH集群，重启cm server和cm agent，启动CDH集群: 12service cloudera-scm-server restartservice cloudera-scm-agent restart 进入cm页面，选择Hosts——&gt;Parcels: 按照提示进行分配安装，激活： 点击集群，添加spark2服务。 在python中正常import pyspark 在/etc/profile中添加如下配置： 过程中遇到的问题 问题1：安装成功后运行pyspark代码报错：启动spark-shell报无法获取资源： 查到的资料： https://stackoverflow.com/questions/30828879/application-report-for-application-state-accepted-never-ends-for-spark-submi/42324377 http://www.cnblogs.com/zlslch/p/6683814.html http://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/spark-shell-stuck/td-p/57603 https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/Endless-INFO-Client-Application-report-for-application-xx/m-p/31461 其他 centOS7 端口占用解决 12ss -lnp|grep 4040kill -9 pid]]></content>
      <categories>
        <category>大数据</category>
        <category>CDH环境配置</category>
      </categories>
      <tags>
        <tag>CDH</tag>
        <tag>华为云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github&hexo博客搭建教程]]></title>
    <url>%2F2018%2F06%2F28%2Fgithub-hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CDH伪分布搭建教程]]></title>
    <url>%2F2018%2F06%2F27%2FCDH%E4%BC%AA%E5%88%86%E5%B8%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[基础环境配置 配置hosts 12sudo vim /etc/hosts 192.168.137.134 master 关闭防火墙 12345sudo systemctl stop firewalld.service #停止firewallsudo systemctl disable firewalld.service #禁止firewall开机启动sudo /etc/sysconfig/selinux SELINUX=disabled #修改sudo setenforce 0 配置无密码登陆 配置本地yum源 1234567891011sudo mkdir /usr/local/src #1. 将CentOS-7-x86_64-DVD-1611.iso拷贝至此sudo mkdir /usr/local/media #2.sudo mount -o loop /usr/local/src/CentOS-7-x86_64-DVD-1611.iso /usr/local/mediaCentOS7/ #3.vim /etc/yum.repos.d/CentOS7-Localsource.repo #4. 并添加一下内容 [CentOS7-Localsource] name=CentOS7 baseurl=file:///usr/local/media/CentOS7 enabled=1 gpgcheck=0sudo yum clean all #5.sudo yum makecache #6. 安装JDK 卸载openJDK 12sudo rpm -qa | grep javasudo yum remove java* 安装Oracle JDK 12345678tar xvf jdk-8u144-linux-x64.gz #路径：/usr/javasudo vim /etc/profile # 末尾添加 export JAVA_HOME=/usr/java/jdk1.8.0_144 export CLASSPATH=.:$CLASSPTAH:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/binsource /etc/profile 安装成功显示 拷贝JDBC驱动包 1cp mysql-connector-java.jar /usr/share/java #路径需要创建 时区配置 查看时区 1date -R 若不为+0800则更改时区为上海 1sudo ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 安装mysql数据库 安装mysql 1234567891011wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpmsudo yum localinstall mysql57-community-release-el7-8.noarch.rpmsudo yum install -y mysql-community-serversudo systemctl start mysqldsudo systemctl enable mysqldsudo systemctl daemon-reload 更改密码 创建cm配置过程中所需的库 配置用户权限 CM安装 在节点安装CM 拷贝CM相关文件至指定目录 安装 12sudo yum localinstall --nogpgcheck *.rpmsudo ./scm_prepare_database.sh mysql -hmaster -uamon -pPassword3# --scm-host master scm scm Password3# CDH服务安装 启动服务 12sudo service cloudera-scm-server startsudo service cloudera-scm-agent start 准备安装文件 安装cdh 登录master:7180，显示如下界面，user/password:admin/admin 选择试用版 选择节点 安装cdh 遇到的问题 问题1： 安装yarn的过程中出错： 解决方案 更改目录权限]]></content>
      <categories>
        <category>大数据</category>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
</search>
