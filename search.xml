<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[YARN中的资源调度]]></title>
    <url>%2F2018%2F07%2F30%2FYARN%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[YARN中有三种调度器可用：FIFO调度器（FIFO Scheduler），容量调度器（Capacity Scheduler）和公平调度器（Fair Scheduler）。 FIFO调度器（FIFO Scheduler）FIFO调度器将应用放置在一个队列中，然后按照提交的顺序（先进先出）运行应用。首先为队列中第一个应用的请求分配资源，第一个应用的请求满足后再依次为队列中下一个应用服务。FIFO调度器优点在于简单易懂，不需要任何配置，但是不适合大集群。大的应用会占用集群中的所有资源，所以每个应用必须等待直到轮到自己运行。 容量调度器（Capacity Scheduler）使用容量调度器时，一个独立的专门队列保证小作业一提交就可以启动，由于队列容量是为那个队列中的作业所保留的，因此这种策略是以整个集群的利用率为代价的。这意味着相比于FIFO调度器，大作业执行的时间要长。容量调度器允许多个组织共享一个集群，每个组织可以分配到全部集群资源的一部分。每个组织被配置一个专门的队列，每个队列被配置为可以使用一定的集群资源。对了可以进一步按层次划分，这样每个组织内的不同用户能够共享该组织队列所分配的资源。在一个队列内，使用FIFO调度策略对应用进行调度。单个作业使用的资源不会超过其队列容量。然而，如果队列中有多个作业，导致队列资源不够用，这时如果仍有可用的空闲资源，那么容量调度器可能会将空余的资源分配给队列中的作业，哪怕这会超过队列容量。这称为“弹性队列”（queue elasticity）。正常操作时，容量调度器不会通过强行终止来抢占容器。因此，如果一个队列一开始资源够用，然后随着需求增长，资源开始不够用时，那么这个队列就只能等着其他队列释放容器资源。一般情况下，为队列设置一个最大容量限制，这样这个队列就不会过多侵占其他队列的容量了。 公平调度器（Fair SCheduler）公平调度器旨在为所有运行的应用公平分配资源。假设有两个用户A和B，分别拥有自己的队列。A启动一个作业，在B没有需求时，A会分配到全部可用资源；当A的作业仍在运行时，B启动一个作业，一段 时间后，每个作业都用到了一半的集群资源。这时，如果B启动了第二个走也且其他作业仍在运行，那么第二个作业将和B的其他作业（这里是第一个）共享资源，因此B的每个作业将占用四分之一的集群资源，而A仍继续占用一半的集群资源。最终的结果是资源在用户之间实现了公平共享。 抢占公平调度器支持抢占功能。所谓抢占，就是允许调度器终止那些占用资源超过了其公平共享份额的队列的容器，这些融资资源释放后可以分配给资源数量低于应得份额的队列。抢占会降低这个集群的效率，因为被终止的Containers需要重新执行。 延迟调度如果一个应用请求某个节点，那么极有可能此时有其他容器正在该节点上运行。实践发现，此时如果等待一小段时间（不超过几秒），能够增鸡杂在所请求的节点上分配到一个容器的机会，从而可以提高集群的效率。这个特性称为延迟调度（delay scheduling）。容量调度器和公平调度器都支持延迟调度。]]></content>
      <categories>
        <category>大数据</category>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN架构原理详解]]></title>
    <url>%2F2018%2F07%2F30%2FYARN%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[YARN基本组件YARN的基本理念是将资源管理和任务调度/监控分成两个独立的服务：全局的资源管理器（ResoureManager）和单独任务的ApplicationMaster。 一个应用既是单独的一个任务，也是一个DAG的任务。YARN总体上仍然是master/slave结构，在整个资源管理框架中，ResourceManager是master，NodeManager是slave。ResourceManager负责对各个NodeManager上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，其负责向ResourceManager申请资源，并要求NodeManager启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此他们之间不会互相影响。 ResourceMangerResourceManager是一个全局资源管理器，负责整个系统的资源管理和分配，包括处理客户端的请求、启动\监控Applicationmaster、监控NodeManager、资源的分配和调度。其主要有两个组件构成：调度器（Scheduler）和应用程序管理器(ApplicationsMaster)。 调度器调度器根据容量、队列等限制条件（如每个队列分配一定的资源、最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。该调度器是一个“纯调度器”，其不再从事任何具体应用程序相关的工作，比如不负责监控或跟踪应用的执行状态等，也不负责重新启动因应用执行失败或硬件故障而产生的失败任务。调度器仅根据各个应用的资源需求进行资源分配，资源分配单位用“资源容器”（Resource Container，简称Container）表示。Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。 应用程序管理器应用程序管理器负责整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。ApplicationMaster管理YARN内运行的应用程序的每个实例。其主要是为应用程序申请资源并进一步分配给内部任务，负责协调来自ResoureManager的资源，并通过NodeManger监控Container的执行和资源的使用情况。NodeManager整个集群中有多个NodeManager，负责每个节点上的资源和使用。功能： 单个节点上的资源管理和任务； 处理来自ResourceManager的命令； 处理来自ApplicationMaster的命令。 NodeManager管理着抽象容器，这些抽象容器代表着一些特定程序使用针对每个节点的资源。NodeManager定时向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态。 ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维度资源。当ApplicationManager向ResourceManager申请资源时，ResourceManager为ApplicationManager返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用Container中的资源。其是一个动态资源划分单位，是根据应用程序的需求动态生成的。 YARN的资源管理 资源调度和隔离是YARN作为一个资源管理系统，最重要最基础的两个功能。资源调度由ResourceManager完成，资源隔离由NodeManager完成。 ResourceManager将某个NodeManager上资源分配给任务后，NodeManager需按照要求为任务提供相应的资源，甚至保证这些资源具有独占性，为任务提供基础和保证。 谈及资源时，主要是内存、CPU、io三种资源。YARN目前仅支持内存、CPU两种资源管理调度。 内存资源多少决定任务的成败；CPU多少决定任务的快慢。 YARN的内存管理YARN允许用户配置每个节点上可用的物理内存资源。因为一个节点上内存会被若干个服务共享，比如一部分给YARN，一部分给hdfs等。YARN配置的只是自己可用的，配置参数主要有： yarn.nodemanager.resource.memory-mb：表示节点上YARN可以使用的物理内存总量； yarn.scheduler.minimum-allocation-mb：单个任务可以使用最小物理内存量； yarn.scheduler.maximum-allocation-mb：单个任务可以申请的最多的内存量。 YARN的CPU管理用户提交应用时，可以指定每个任务需要的虚拟CPU数。相关配置参数如下： yarn.nodemanager.resource.cpu-vcores：表示该节点上可以使用的虚拟CPU个数； yarn.scheduler.minimum-allocation-vcores：表示单个任务可以申请的最小CPU个数； yarn.scheduler.maximum-allocation-vcores：表示单个任务可以申请最多虚拟CPU个数。 YARN程序运行机制首先，客户端联系ResoureManager，要求它运行一个ApplicationManager进程；然后，ResoureManager找到一个能够在Container中启动ApplicationMaster的NodeManager（步骤2a和2b）。ApplicationMaster一旦运行起来后能做些什么依赖于应用本身。有可能是在所处的容器中简单运行一个计算，并将结果返回给客户端；或者向ResoureManager请求更多的Container（步骤3），以用于运行一个分布式计算（步骤4a和4b）。YARN本身不会为应用的各个部分彼此间通信提供任何手段。大多数重要的YARN应用使用某种形式的远程通信机制（例如Hadoop的RPC层）来向客户端传递状态更新和返回结果。]]></content>
      <categories>
        <category>大数据</category>
        <category>YARN</category>
      </categories>
      <tags>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala之模式匹配]]></title>
    <url>%2F2018%2F07%2F25%2Fscala%E4%B9%8B%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[简介模式可以当做对某个类型，其内部数据在结构上抽象出来的表达式。scala中模式匹配使用match关键字。match可以当做是java风格的switch的广义化。但是有三个区别： scala中的match是一个表达式，可以匹配各种情况； scala的可选分支不会贯穿到下一个case； 如果一个模式都没匹配上，会抛出MatchError的异常。一般会添加什么都不做的缺省case。 模式的种类通配模式通配模式(_)可以匹配任何对象。1234567def judgeGrade(grade:String): Unit = &#123; grade match &#123; case _ =&gt; println("others") &#125;&#125;judgeGrade("hello") //others 常量模式常量模式仅匹配自己。任何字面量都可以作为常量模式使用。1234567891011def judgeGrade(grade:String): Unit = &#123; grade match &#123; case "A" =&gt; println("A") case "B" =&gt; println("B") case "C" =&gt; println("C") case _ =&gt; println("others") &#125;&#125;judgeGrade("A") //AjudgeGrade("B") //B 变量模式变量模式匹配任何对象。这一点和通配模式相同。不同之处在于变量模式会将对应的变量绑定在匹配的对象上。之后可以用这个变量对对象作进一步的处理。123456789def matchSomething(something: Int): Unit = &#123; something match &#123; case 0 =&gt; println("zero") case something =&gt; println("not zero: " + something) &#125;&#125;matchSomething(0) // zeromatchSomething(1) // not zero: 1 上例中something可以匹配任何除0外的Int值。另外例子：12345678910import math.&#123;E, Pi&#125;def matchPi(x: Double): Unit = &#123; x match &#123; case Pi =&gt; println("Pi: " + Pi) case _ =&gt; println("OK") &#125;&#125;matchPi(E) // OK 可以看出E并不匹配Pi。scala采用了一个简单的词法来区分：一个以小写字母打头的简单名称会被当做模式变量处理，所有其他引用都是常量。12345678910import math.&#123;E, Pi&#125;def matchpi(x: Double): Unit = &#123; x match &#123; case pi =&gt; println("Pi: " + pi) case _ =&gt; println("OK") &#125;&#125;matchPi(E) // Pi: 2.718281828459045 构造方法模式如例所示。假设匹配的是一个样例类（case class），这样模式将首先检查被匹配的对象是否是以这个名称命名的样例类的实例，然后再检查这个对象的改造方法参数是否匹配这些额外给出的模式。如果不是样例类，则需要定义伴生对象并实现unapply方法。12345678910111213141516case class Person(name: String, age: Int)object ConstructorPattern &#123; def main(args: Array[String]): Unit = &#123; val p = Person("stm", 25) def constructorPattern(p: Person) = &#123; p match &#123; case Person(name, age) =&gt; println("name: " + name + ", age: " + age) case _ =&gt; "other" &#125; &#125; constructorPattern(p) // name: stm, age: 25 &#125;&#125; 序列模式可以和Array、List等序列类型匹配。在模式中可以给出任意数量的元素。其原理也是通过case class。_*可以匹配剩余元素，包括0个。123456789101112131415object SequencePattern &#123; def main(args: Array[String]): Unit = &#123; val list = List("spark", "hive") val arr = Array("scala", "java", "python") def sequencePattern(p: Any) = p match &#123; case List(_, second, _*) =&gt; println(second) case Array(first, second, _*) =&gt; println(first + ", " + second) case _ =&gt; println("other") &#125; sequencePattern(list) // hive sequencePattern(arr) // scala, java &#125;&#125; 元组模式元组模式用于匹配scala中的元组内容。_*不适用于元组。1234567891011121314object TuplePattern &#123; def main(args: Array[String]): Unit = &#123; val tuple1 = ("spark", "hive", "hadoop") val tuple2 =("java", "python") def tuplePattern(t:Any) = t match &#123; case (one, _, _) =&gt; println(one) case (one, two) =&gt; println(one + ", " + two) case _ =&gt; println("other") &#125; tuplePattern(tuple1) // spark tuplePattern(tuple2) // java, python &#125;&#125; 类型模式可以用来替代类型测试和类型转换。Map[_, _]匹配任意Map。123456789101112object TypePattern &#123; def main(args: Array[String]): Unit = &#123; def typePattern(t:Any) = t match &#123; case t :String =&gt; println("t.length: " + t.length) case t :Map[_, _] =&gt; println("t.size: " + t.size) case _ =&gt; println("other") &#125; typePattern("hello") // t.length: 5 typePattern(Map(1-&gt;'a', 2-&gt;'b')) // t.size: 2 &#125;&#125; 类型擦除java和scala中都采用了擦除式的泛型。即在运行中无法判定某个给定的Map对象是用两个Int类型参数创建还是其他类型。但是数组除外。123456789101112131415161718192021object TypeErasure &#123; def main(args: Array[String]): Unit = &#123; val m1 = Map(1 -&gt; 1, 2 -&gt; 2) val m2 = Map(1 -&gt; "a", 2-&gt; "b") def isIntIntMap(x:Any) = x match &#123; case m:Map[Int, Int] =&gt; println(true) case _ =&gt; println(false) &#125; isIntIntMap(m1) //true isIntIntMap(m2) //true def isStringArray(x:Any) = x match &#123; case x : Array[String] =&gt; println(true) case _ =&gt; println(false) &#125; isStringArray(Array("hello")) //true isStringArray(Array(33)) //false &#125;&#125; 变量绑定模式可以对任何其他模式添加变量。只需要写下变量名、一个@符合模式本身，就得到一个变量绑定模式。如果匹配成功，就将匹配的对象赋值给这个变量。123456789101112object VariableBindingPattern &#123; def main(args: Array[String]): Unit = &#123; val t = List(List(1,2), List(4,5,6)) def variableBindingPattern(t:Any) = t match &#123; case List(_, e@List(_, _, _)) =&gt; println(e) case _ =&gt; println("other") &#125; variableBindingPattern(t) //List(4, 5, 6) &#125;&#125; 模式守卫使用if表达式。1234567891011121314object PatternGuards &#123; def main(args: Array[String]): Unit = &#123; val list1 = List(1, 2, 3) val list2 = List(4, 5, 6) def patternGuards(x: Any) = x match &#123; case List(first, _*) if first == 1 =&gt; println(x) case _ =&gt; println("others") &#125; patternGuards(list1) //List(1, 2, 3) patternGuards(list2) //others &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala case 之类与对象]]></title>
    <url>%2F2018%2F07%2F24%2Fscala-case-%E4%B9%8B%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍case class与普通class的一些区别。以及case class与case object的异同点。 case class与class的区别 Person.scala源码，运行后生成Person$.class和Person.class两个文件。12345678case class Person(age:Int, name:String)object Person &#123; def main(args: Array[String]): Unit = &#123; val person = Person(25,"stm") println(person.toString) &#125;&#125; 对Person.class反编译如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import scala.Function1;import scala.Option;import scala.Product;package com.stm.datastructures.caseClass;class;import scala.Serializable;import scala.Tuple2;import scala.collection.Iterator;import scala.reflect.ScalaSignature;import scala.runtime.BoxesRunTime;import scala.runtime.ScalaRunTime.;import scala.runtime.Statics;@ScalaSignaturepublic class Person implements Product, Serializable &#123; private final int age; private final String name; public Person(int age, String name) &#123; Product.class.$init$(this); &#125; public boolean equals(Object x$1) &#123; if (this != x$1) &#123; Object localObject = x$1; int i; if ((localObject instanceof Person)) &#123; i = 1; &#125; else &#123; i = 0; &#125; if (i == 0) &#123; break label96; &#125; Person localPerson = (Person) x$1; if (age() == localPerson.age()) &#123; str = localPerson.name(); String tmp54_44 = name(); if (tmp54_44 == null) &#123; tmp54_44; if (str == null) &#123; break label75; &#125; tmpTernaryOp = tmp54_44; break label88; &#125; &#125; &#125; &#125; public String toString() &#123; return ScalaRunTime..MODULE$._toString(this); &#125; public int hashCode() &#123; int i = -889275714; i = Statics.mix(i, age()); i = Statics.mix(i, Statics.anyHash(name())); return Statics.finalizeHash(i, 2); &#125; public boolean canEqual(Object x$1) &#123; return x$1 instanceof Person; &#125; public Iterator&lt;Object&gt; productIterator() &#123; return ScalaRunTime..MODULE$.typedProductIterator(this); &#125; public Object productElement(int x$1) &#123; int i = x$1; switch (i) &#123; default: throw new IndexOutOfBoundsException(BoxesRunTime.boxToInteger(x$1).toString()); case 1: break; &#125; return BoxesRunTime.boxToInteger(age()); &#125; public int productArity() &#123; return 2; &#125; public String productPrefix() &#123; return "Person"; &#125; public String copy$default$2() &#123; return name(); &#125; public int copy$default$1() &#123; return age(); &#125; public Person copy(int age, String name) &#123; return new Person(age, name); &#125; public String name() &#123; return this.name; &#125; public int age() &#123; return this.age; &#125; public static Function1&lt;Object, Function1&lt;String, Person&gt;&gt; curried() &#123; return Person..MODULE$.curried(); &#125; public static Function1&lt;Tuple2&lt;Object, String&gt;, Person&gt; tupled() &#123; return Person..MODULE$.tupled(); &#125; public static Person apply(int paramInt, String paramString) &#123; return Person..MODULE$.apply(paramInt, paramString); &#125; public static Option&lt;Tuple2&lt;Object, String&gt;&gt; unapply(Person paramPerson) &#123; return Person..MODULE$.unapply(paramPerson); &#125;&#125; 对Person$.class编译如下：12345678910111213141516171819202122232425262728293031323334353637import scala.Option;import scala.Serializable;import scala.Some;import scala.Tuple2;import scala.runtime.AbstractFunction2;import scala.runtime.BoxesRunTime;public final class Person$ extends AbstractFunction2&lt;Object, String, Person&gt; implements Serializable &#123; public static final MODULE$; private Person$() &#123; MODULE$ = this; &#125; private Object readResolve() &#123; return MODULE$; &#125; public Option&lt;Tuple2&lt;Object, String&gt;&gt; unapply(Person x$0) &#123; return x$0 == null ? None..MODULE$: new Some(new Tuple2(BoxesRunTime.boxToInteger(x$0.age()), x$0.name())); &#125; public Person apply(int age, String name) &#123; return new Person(age, name); &#125; public final String toString() &#123; return "Person"; &#125; static &#123; new (); &#125;&#125; Student.scala源码，运行后生成Student$.class和Student.class两个文件。12345678class Student(val age:Int, val name:String)&#123;&#125;object Student&#123; def main(args: Array[String]): Unit = &#123; val stu = new Student(25, "stm") println(stu) &#125;&#125; 对Student.class反编译如下:12345678910111213141516171819202122import scala.reflect.ScalaSignature;@ScalaSignaturepublic class Student &#123; private final int age; private final String name; public Student(int age, String name) &#123; &#125; public String name() &#123; return this.name; &#125; public int age() &#123; return this.age; &#125; public static void main(String[] paramArrayOfString) &#123; Student..MODULE$.main(paramArrayOfString); &#125;&#125; 对Student$.class反编译如下:123456789101112131415161718import scala.Predef.;public final class Student$ &#123; public static final MODULE$; static &#123; new (); &#125; public void main(String[] args) &#123; Student stu = new Student(25, "stm"); Predef..MODULE$.println(stu); &#125; private Student$() &#123; MODULE$ = this; &#125;&#125; 从编译结果，可以看出一下几点： 生成了apply方法，可以直接把对象当做方法使用。 1val person = Person(25,"stm") 继承了Product和Serializable。 age和name是用final修饰的。 默认实现了toString。 实现了name()和age()方法。（参数列表中的参数都隐式获得一个val作为前缀） 12println(person1.age) //25println(person1.name) //stm 重写了equals方法，比较structure而不是reference: 123val person1 = Person(25, "stm")val person2 = Person(25, "stm")println(person1 == person2) //true case object与case class区别 case object反编译后没有apply和unapply方法，因为caes object没有参数。 总结 case class和case object本质没区别，有参使用case class，无参使用case object; case class和case object增加了继承和方法。 case class和case object支持模式匹配。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark之RDD基础简介]]></title>
    <url>%2F2018%2F07%2F19%2Fspark%E4%B9%8BRDD%E5%9F%BA%E7%A1%80%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[RDD简介RDD全称为弹性分布式数据集（Resilient Distributed Dataset），是spark的编程模型，是MapReduce模型的扩展和延伸，可以在并行计算阶段高效地进行数据共享。 RDD基础RDD类型RDD主要分为以下四种类型： 创建操作：用于RDD创建工作。主要有两种方法：来自于内存集合和外部存储系统；通过转换操作生成的RDD。 转换操作：将RDD通过一定的操作变换成新的RDD。 控制操作：进行RDD持久化。让RDD根据不同的存储策略保存在内存或者磁盘中。 行动操作：能够触发spark运行的操作。spark中行动操作分为两类：一类的操作结果变成scala集合或变量，另一类的操作是将RDD保存到外部文件系统或者数据库中。 创建RDD 并行化集合创建操作 使用SparkContext的parallelize方法，在一个已经存在的scala集合上创建。集合的对象将会被复制，创建出一个可以被并行操作的分布式数据集。 外部存储创建操作 spark可以将任何Hadoop所支持的存储资源转换成RDD。 RDD依赖关系主要分为窄依赖和宽依赖： 窄依赖：每个父RDD的分区都至多被一个子RDD的分区使用。 宽依赖：多个子RDD的分区依赖一个父RDD的分区。 读取文件 wordmap的依赖关系是OneToOneDependency，属于窄依赖 使用reduceByKey操作对单词进行计数 wordreduce的依赖关系是ShuffleDependency，属于宽依赖 RDD的分区数RDD划分很多的分区（partition）分布到集群的节点中，分区的多少涉及对这个RDD进行并行计算的粒度。分区是个逻辑概念。用户可以在读取文件时指定分区数目。默认数值是改程序所分配到的CPU核数，如果从HDFS进行创建，则默认为文件的副本数。 RDD分区计算（Iterator）spark中RDD计算是以分区为单位的，而且计算函数都是对迭代器复合，不需要保存每次计算的结果。分区计算一般使用mapPartitions等操作进行。1def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] f为输入函数，处理每个分区里面的内容。 函数iterfunc把分区中一个元素和他的下个元素组成一个Turple RDD分区函数（Partitioner）spark默认提供两种划分器：哈希分区划分器（HashPartitioner）和范围分区划分器（RangePartitioner），且Partitioner只存在(K, V)类型的RDD中，对于非(K, V)类型的RDD，其Partitioner值为None。 参数4是group_rdd最终拥有的分区数 RDD基本转换操作 map： flatMap： distinct： coalesce：对RDD重新分区。第一个参数是重分区的数目，第二个参数为是否进行shuffle，默认为false。如果重分区的数目大于原分区，则需要设为True。 repartition：是coalesce第二个参数为True时的实现。 randomSplit： union：结果不去重。 mapPartitions mapPartitionsWithIndex zip：将两个同样分区RDD进行合并，键值分别对照组合。分区不同的两个RDD会报异常。 RDD键值转换操作 reduceByKey：将RDD[K, V]中每个K对应的V根据映射函数进行计算。 reduceByKeyLocally：同reduceByKey，不过是将结果映射到一个Map[K, V]中。 join、fullOuterJoin、leftOuterJoin、rightOuterJoin： join 内连接操作： leftOuterJoin 左连接操作： rightOuterJoin 右连接操作： RDD行动操作 aggregate： 进行aggregate操作：先在每个分区中迭代执行 (x:Int, y:Int) =&gt; x + y，并且zeroValue为1，即分区1中为1+5+4+3+2+1=16，分区2中为1+10+9+8+7+6=41再将两个分区进行合并，1+16+41=58 总结本文主要对RDD的四种操作进行了简单汇总，后续将对一些操作函数进行扩充完善。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala中的map和flatMap的区别]]></title>
    <url>%2F2018%2F07%2F16%2Fscala%E4%B8%AD%E7%9A%84map%E5%92%8CflatMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Titan图形数据库学习笔记（一）]]></title>
    <url>%2F2018%2F07%2F10%2FTitan%E5%9B%BE%E5%BD%A2%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Titan数据库简介 Titan 是一个可扩展的图形数据库，结合HBase、Cassandra、BerkeleyDB提供存储功能，ES、Lucene、Solar提供索引功能，可利用Hadoop计算框架对图数据进行分析、统计。经过优化，可用于存储和索引分布于多节点集群的百亿级顶点和边的图，同时，Titan又是一个事务数据库，可以支持数千个并发用户实时执行复杂图形遍历。 框架Titan是一个图形数据库引擎，其本身专注于紧凑图表序列化，丰富的图形数据库建模和高效的查询执行。其框架主要如下: Titan与底层磁盘之间有多个存储和索引适配器： 存储： Cassandra HBase BerkeleyDB 索引： Elasticsearch Solr Lucene Titan提供了三种交互式接口： TitanGraph API TinkPop API Management API Titan数据库环境配置 下载titan-1.0.0-hadoop2.zip解压。 配置文件 进入conf/es文件夹，配置elasticsearch.yml，截取如下： 12345678path.data: db/es/datapath.work: db/es/workpath.logs: logpath.plugins: bin/espluginsnetwork.host: 22.144.110.125transport.tcp.port: 9300http.port: 9200discovery.zen.ping.unicast.hosts: ["hadoop1","hadoop2","hadoop3","hadoop4","hadoop5","hadoop6"] 进入conf文件夹，配置titan-hbase-es.properties，截取如下： 123456789storage.backend=hbasestorage.hostname=hadoop1, hadoop2, hadoop3, hadoop4, hadoop5, hadoop6cache.db-cache = truecache.db-cache-clean-wait = 20cache.db-cache-time = 180000cache.db-cache-size = 0.5index.search.backend=elasticsearchindex.search.hostname=hadoop5index.search.elasticsearch.client-only=true 启动 进入bin目录，运行elasticsearch： 重开终端，运行gremlin.sh： 输入以下脚本： 123graph = TitanFactory.open('../conf/titan-hbase-es.properties')g = graph.traversal()saturn = g.V().has('name', 'saturn').next() 参考资料 http://s3.thinkaurelius.com/docs/titan/1.0.0/getting-started.html http://database.51cto.com/art/201804/570147.htm https://blog.csdn.net/samhacker/article/details/39721131]]></content>
      <categories>
        <category>图形数据库</category>
      </categories>
      <tags>
        <tag>Titan</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于华为云的CDH配置介绍]]></title>
    <url>%2F2018%2F06%2F28%2F%E5%9F%BA%E4%BA%8E%E5%8D%8E%E4%B8%BA%E4%BA%91%E7%9A%84CDH%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[基础环境配置 说明：参考文章CDH伪分布搭建教程。此处作为补充。整个操作在root用户下进行。 安装图形化界面（非必须） 1234yum groupinstall "X Window System" #1yum groupinstall "GNOME Desktop" "Graphical Administration Tools" #2ln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target #3reboot #4 安装Anaconda 说明：考虑到多用户使用存在的权限问题，将安装路径设置为/usr/local/anaconda3。 更改权限： 1chmod 777 Anaconda3-5.1.0-Linux-x86_64.sh 执行安装，安装过程中手动添加安装路径/usr/local/anaconda3，选择添加path至~/.bashrc中，不安装Mircosoft vsCode。 1sh Anaconda3-5.1.0-Linux-x86_64.sh 添加spyder链接： 方便在MobaXterm中打开图形化界面。 1ln -s /usr/local/anaconda3/bin/spyder /usr/bin/spyder 重启： 1reboot 添加hive服务 添加hive所需库： 1234567-- 在mysql中执行create user 'hive'@'%' identified by 'Password3#';grant all on *.* to 'hive'@'%' identified by 'Password3#';flush privileges;create database metastore;alter database hive character set latin1; 添加zookeeper服务 添加hive服务，数据库选择metastore 添加spark2服务 准备文件 spark2 Anaconda 安装步骤 将SPARK2_ON_YARN-2.1.0.cloudera2.jar拷贝到/opt/cloudera/csd，并且更改用户权限chown cloudera-scm:cloudera-scm 将其他文件拷贝到/opt/cloudera/parcel-repo 关闭CDH集群，重启cm server和cm agent，启动CDH集群: 12service cloudera-scm-server restartservice cloudera-scm-agent restart 进入cm页面，选择Hosts——&gt;Parcels: 按照提示进行分配安装，激活： 点击集群，添加spark2服务。 在python中正常import pyspark 在/etc/profile中添加如下配置： 过程中遇到的问题 问题1：安装成功后运行pyspark代码报错：启动spark-shell报无法获取资源： 查到的资料： https://stackoverflow.com/questions/30828879/application-report-for-application-state-accepted-never-ends-for-spark-submi/42324377 http://www.cnblogs.com/zlslch/p/6683814.html http://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/spark-shell-stuck/td-p/57603 https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/Endless-INFO-Client-Application-report-for-application-xx/m-p/31461 其他 centOS7 端口占用解决 12ss -lnp|grep 4040kill -9 pid]]></content>
      <categories>
        <category>CDH环境配置</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>CDH</tag>
        <tag>华为云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github&hexo博客搭建教程]]></title>
    <url>%2F2018%2F06%2F28%2Fgithub-hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CDH伪分布搭建教程]]></title>
    <url>%2F2018%2F06%2F27%2FCDH%E4%BC%AA%E5%88%86%E5%B8%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[基础环境配置 配置hosts 12sudo vim /etc/hosts 192.168.137.134 master 关闭防火墙 12345sudo systemctl stop firewalld.service #停止firewallsudo systemctl disable firewalld.service #禁止firewall开机启动sudo /etc/sysconfig/selinux SELINUX=disabled #修改sudo setenforce 0 配置无密码登陆 配置本地yum源 1234567891011sudo mkdir /usr/local/src #1. 将CentOS-7-x86_64-DVD-1611.iso拷贝至此sudo mkdir /usr/local/media #2.sudo mount -o loop /usr/local/src/CentOS-7-x86_64-DVD-1611.iso /usr/local/mediaCentOS7/ #3.vim /etc/yum.repos.d/CentOS7-Localsource.repo #4. 并添加一下内容 [CentOS7-Localsource] name=CentOS7 baseurl=file:///usr/local/media/CentOS7 enabled=1 gpgcheck=0sudo yum clean all #5.sudo yum makecache #6. 安装JDK 卸载openJDK 12sudo rpm -qa | grep javasudo yum remove java* 安装Oracle JDK 12345678tar xvf jdk-8u144-linux-x64.gz #路径：/usr/javasudo vim /etc/profile # 末尾添加 export JAVA_HOME=/usr/java/jdk1.8.0_144 export CLASSPATH=.:$CLASSPTAH:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/binsource /etc/profile 安装成功显示 拷贝JDBC驱动包 1cp mysql-connector-java.jar /usr/share/java #路径需要创建 时区配置 查看时区 1date -R 若不为+0800则更改时区为上海 1sudo ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 安装mysql数据库 安装mysql 1234567891011wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpmsudo yum localinstall mysql57-community-release-el7-8.noarch.rpmsudo yum install -y mysql-community-serversudo systemctl start mysqldsudo systemctl enable mysqldsudo systemctl daemon-reload 更改密码 创建cm配置过程中所需的库 配置用户权限 CM安装 在节点安装CM 拷贝CM相关文件至指定目录 安装 12sudo yum localinstall --nogpgcheck *.rpmsudo ./scm_prepare_database.sh mysql -hmaster -uamon -pPassword3# --scm-host master scm scm Password3# CDH服务安装 启动服务 12sudo service cloudera-scm-server startsudo service cloudera-scm-agent start 准备安装文件 安装cdh 登录master:7180，显示如下界面，user/password:admin/admin 选择试用版 选择节点 安装cdh 遇到的问题 问题1： 安装yarn的过程中出错： 解决方案 更改目录权限]]></content>
      <categories>
        <category>CDH环境配置</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
</search>
